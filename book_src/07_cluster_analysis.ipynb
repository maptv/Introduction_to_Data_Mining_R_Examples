{
 "cells": [
  {
   "cell_type": "raw",
   "id": "83bd444f",
   "metadata": {},
   "source": [
    "---\n",
    "editor_options: \n",
    "  markdown: \n",
    "    wrap: 72\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb1dd2",
   "metadata": {},
   "source": [
    "# Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8086f",
   "metadata": {
    "name": "setup_07",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pkgs <- sort(c(\n",
    "'tidyverse',\n",
    "'factoextra',\n",
    "'dbscan',\n",
    "'cluster',\n",
    "'mclust',\n",
    "'kernlab',\n",
    "'e1071',\n",
    "'scatterpie',\n",
    "'fpc',\n",
    "'seriation',\n",
    "'mlbench',\n",
    "'GGally'\n",
    "))\n",
    "  \n",
    "lapply(pkgs, function(pkg) {\n",
    "  if (system.file(package = pkg) == '') install.packages(pkg)\n",
    "})\n",
    "\n",
    "all_pkgs <- union(all_pkgs, pkgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7882a",
   "metadata": {},
   "source": [
    "**Packages used for this chapter:** `r format_pkgs(pkgs)`\n",
    "\n",
    "You can read the free sample chapter from the textbook [@Tan2005]:\n",
    "[Chapter 7. Cluster Analysis: Basic Concepts and\n",
    "Algorithms](https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf)\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802aa85",
   "metadata": {},
   "source": [
    "We will use here a small and very clean dataset called Ruspini which is\n",
    "included in the R package **cluster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(ruspini, package = \"cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8144bb4",
   "metadata": {},
   "source": [
    "The Ruspini data set, consisting of 75 points in four groups that is\n",
    "popular for illustrating clustering techniques. It is a very simple data\n",
    "set with well separated clusters. The original dataset has the points\n",
    "ordered by group. We can shuffle the data (rows) using `sample_frac`\n",
    "which samples by default 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7da1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruspini <- as_tibble(ruspini) %>% sample_frac()\n",
    "ruspini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecab8b",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03896199",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(ruspini, aes(x = x, y = y)) + geom_point()\n",
    "\n",
    "summary(ruspini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a512c7",
   "metadata": {},
   "source": [
    "For most clustering algorithms it is necessary to handle missing values\n",
    "and outliers (e.g., remove the observations). For details see Section\n",
    "\"Outlier removal\" below. This data set has not missing values or strong\n",
    "outlier and looks like it has some very clear groups.\n",
    "\n",
    "### Scale data\n",
    "\n",
    "Clustering algorithms use distances and the variables with the largest\n",
    "number range will dominate distance calculation. The summary above shows\n",
    "that this is not an issue for the Ruspini dataset with both, x and y,\n",
    "being roughly between 0 and 150. Most data analysts will still scale\n",
    "each column in the data to zero mean and unit standard deviation\n",
    "(z-scores). *Note:* The standard `scale()` function scales a whole data\n",
    "matrix so we implement a function for a single vector and apply it to\n",
    "all numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a40348",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I use this till tidyverse implements a scale function\n",
    "scale_numeric <- function(x) x %>% mutate_if(is.numeric, function(y) as.vector(scale(y)))\n",
    "\n",
    "ruspini_scaled <- ruspini %>% scale_numeric()\n",
    "summary(ruspini_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6384d2",
   "metadata": {},
   "source": [
    "After scaling, most z-scores will fall in the range $[-3,3]$ (z-scores\n",
    "are measured in standard deviations from the mean), where $0$ means\n",
    "average.\n",
    "\n",
    "## Clustering methods\n",
    "\n",
    "### k-means Clustering\n",
    "\n",
    "[k-means](https://en.wikipedia.org/wiki/K-means_clustering) implicitly\n",
    "assumes Euclidean distances. We use $k = 4$ clusters and run the\n",
    "algorithm 10 times with random initialized centroids. The best result is\n",
    "returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "km <- kmeans(ruspini_scaled, centers = 4, nstart = 10)\n",
    "km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923914e",
   "metadata": {},
   "source": [
    "`km` is an R object implemented as a list. The clustering vector\n",
    "contains the cluster assignment for each data row and can be accessed\n",
    "using `km$cluster`. I add the cluster assignment as a column to the\n",
    "scaled dataset (I make it a factor since it represents a nominal label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruspini_clustered <- ruspini_scaled %>% add_column(cluster = factor(km$cluster))\n",
    "ruspini_clustered\n",
    "\n",
    "ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aff407",
   "metadata": {},
   "source": [
    "Add the centroids to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids <- as_tibble(km$centers, rownames = \"cluster\")\n",
    "centroids\n",
    "\n",
    "ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() +\n",
    "  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765804b0",
   "metadata": {},
   "source": [
    "Use the `factoextra` package for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(factoextra)\n",
    "fviz_cluster(km, data = ruspini_scaled, centroids = TRUE, repel = TRUE, ellipse.type = \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2555533a",
   "metadata": {},
   "source": [
    "#### Inspect clusters\n",
    "\n",
    "We inspect the clusters created by the 4-cluster k-means solution. The\n",
    "following code can be adapted to be used for other clustering methods.\n",
    "\n",
    "##### Cluster Profiles\n",
    "\n",
    "Inspect the centroids with horizontal bar charts organized by cluster.\n",
    "To group the plots by cluster, we have to change the data format to the\n",
    "\"long\"-format using a pivot operation. I use colors to match the\n",
    "clusters in the scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(pivot_longer(centroids, cols = c(x, y), names_to = \"feature\"),\n",
    "  aes(x = value, y = feature, fill = cluster)) +\n",
    "  geom_bar(stat = \"identity\") +\n",
    "  facet_grid(rows = vars(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6618c",
   "metadata": {},
   "source": [
    "##### Extract a single cluster\n",
    "\n",
    "You need is to filter the rows corresponding to the cluster index. The\n",
    "next example calculates summary statistics and then plots all data\n",
    "points of cluster 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1 <- ruspini_clustered %>% filter(cluster == 1)\n",
    "cluster1\n",
    "summary(cluster1)\n",
    "\n",
    "ggplot(cluster1, aes(x = x, y = y)) + geom_point() +\n",
    "  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb8966",
   "metadata": {},
   "source": [
    "What happens if we try to cluster with 8 centers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_cluster(kmeans(ruspini_scaled, centers = 8), data = ruspini_scaled,\n",
    "  centroids = TRUE,  geom = \"point\", ellipse.type = \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd263d",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering starts with a distance matrix. `dist()` defaults\n",
    "to method=\"Euclidean\". **Note:** Distance matrices become very large\n",
    "quickly (size and time complexity is $O(n^2)$ where $n$ is the number if\n",
    "data points). It is only possible to calculate and store the matrix for\n",
    "small data sets (maybe a few hundred thousand data points) in main\n",
    "memory. If your data is too large then you can use sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "d <- dist(ruspini_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065d56d",
   "metadata": {},
   "source": [
    "`hclust()` implements [agglomerative hierarchical\n",
    "clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering). We\n",
    "cluster using complete link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3384b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc <- hclust(d, method = \"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353721d9",
   "metadata": {},
   "source": [
    "Hierarchical clustering does not return cluster assignments but a\n",
    "dendrogram. The standard plot function plots the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152467cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae19747",
   "metadata": {},
   "source": [
    "Use `factoextra` (ggplot version). We can specify the number of clusters\n",
    "to visualize how the dendrogram will be cut into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_dend(hc, k = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7a95d",
   "metadata": {},
   "source": [
    "More plotting options for dendrograms, including plotting parts of large\n",
    "dendrograms can be found [here.](https://rpubs.com/gaston/dendrograms)\n",
    "\n",
    "Extract cluster assignments by cutting the dendrogram into four parts\n",
    "and add the cluster id to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f903b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters <- cutree(hc, k = 4)\n",
    "cluster_complete <- ruspini_scaled %>%\n",
    "  add_column(cluster = factor(clusters))\n",
    "cluster_complete\n",
    "\n",
    "ggplot(cluster_complete, aes(x, y, color = cluster)) +\n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a812ceee",
   "metadata": {},
   "source": [
    "Try 8 clusters (Note: `fviz_cluster` needs a list with data and the\n",
    "cluster labels for hclust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc, k = 8)), geom = \"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c752b74",
   "metadata": {},
   "source": [
    "Clustering with single link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b0cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_single <- hclust(d, method = \"single\")\n",
    "fviz_dend(hc_single, k = 4)\n",
    "\n",
    "fviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc_single, k = 4)), geom = \"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346494b",
   "metadata": {},
   "source": [
    "### Density-based clustering with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aef81c",
   "metadata": {},
   "source": [
    "[DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) stands for \"Density-Based\n",
    "Spatial Clustering of Applications with Noise.\" It groups together\n",
    "points that are closely packed together and treats points in low-density\n",
    "regions as outliers.\n",
    "\n",
    "**Parameters:** minPts defines how many points in the epsilon\n",
    "neighborhood are needed to make a point a core point. It is often chosen\n",
    "as a smoothing parameter. I use here minPts = 4.\n",
    "\n",
    "To decide on epsilon, the knee in the kNN distance plot is often used.\n",
    "Note that minPts contains the point itself, while the k-nearest neighbor\n",
    "does not. We therefore have to use k = minPts - 1! The knee is around\n",
    "eps = .32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "kNNdistplot(ruspini_scaled, k = 3)\n",
    "abline(h = .32, col = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed75b5e",
   "metadata": {},
   "source": [
    "run dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96328319",
   "metadata": {},
   "outputs": [],
   "source": [
    "db <- dbscan(ruspini_scaled, eps = .32, minPts = 4)\n",
    "db\n",
    "str(db)\n",
    "\n",
    "ggplot(ruspini_scaled %>% add_column(cluster = factor(db$cluster)),\n",
    "  aes(x, y, color = cluster)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92aa9f7",
   "metadata": {},
   "source": [
    "**Note:** Cluster 0 represents outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db721709",
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_cluster(db, ruspini_scaled, geom = \"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02339a",
   "metadata": {},
   "source": [
    "Play with eps (neighborhood size) and MinPts (minimum of points needed\n",
    "for core cluster)\n",
    "\n",
    "### Partitioning Around Medoids (PAM)\n",
    "\n",
    "[PAM](https://en.wikipedia.org/wiki/K-medoids) tries to solve the\n",
    "$k$-medoids problem. The problem is similar to $k$-means, but uses\n",
    "medoids instead of centroids to represent clusters. Like hierarchical\n",
    "clustering, it typically works with precomputed distance matrix. An\n",
    "advantage is that you can use any distance metric not just Euclidean\n",
    "distances. **Note:** The medoid is the most central data point in the\n",
    "middle of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(cluster)\n",
    "\n",
    "d <- dist(ruspini_scaled)\n",
    "str(d)\n",
    "\n",
    "p <- pam(d, k = 4)\n",
    "p\n",
    "\n",
    "ruspini_clustered <- ruspini_scaled %>% add_column(cluster = factor(p$cluster))\n",
    "\n",
    "medoids <- as_tibble(ruspini_scaled[p$medoids, ], rownames = \"cluster\")\n",
    "medoids\n",
    "\n",
    "ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() +\n",
    "  geom_point(data = medoids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)\n",
    "\n",
    "## __Note:__ `fviz_cluster` needs the original data.\n",
    "fviz_cluster(c(p, list(data = ruspini_scaled)), geom = \"point\", ellipse.type = \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adc52b",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(mclust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448bae4",
   "metadata": {},
   "source": [
    "[Gaussian mixture\n",
    "models](https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model)\n",
    "assume that the data set is the result of drawing data from a set of\n",
    "Gaussian distributions where each distribution represents a cluster.\n",
    "Estimation algorithms try to identify the location parameters of the\n",
    "distributions and thus can be used to find clusters. `Mclust()` uses\n",
    "Bayesian Information Criterion (BIC) to find the number of clusters\n",
    "(model selection). BIC uses the likelihood and a penalty term to guard\n",
    "against overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a496144",
   "metadata": {},
   "outputs": [],
   "source": [
    "m <- Mclust(ruspini_scaled)\n",
    "summary(m)\n",
    "plot(m, what = \"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53721c34",
   "metadata": {},
   "source": [
    "Rerun with a fixed number of 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "m <- Mclust(ruspini_scaled, G=4)\n",
    "summary(m)\n",
    "plot(m, what = \"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc2c83",
   "metadata": {},
   "source": [
    "### Spectral clustering\n",
    "\n",
    "[Spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering)\n",
    "works by embedding the data points of the partitioning problem into the\n",
    "subspace of the k largest eigenvectors of a normalized affinity/kernel\n",
    "matrix. Then uses a simple clustering method like k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"kernlab\")\n",
    "\n",
    "cluster_spec <- specc(as.matrix(ruspini_scaled), centers = 4)\n",
    "cluster_spec\n",
    "\n",
    "ggplot(ruspini_scaled %>% add_column(cluster = factor(cluster_spec)),\n",
    "  aes(x, y, color = cluster)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb97b7",
   "metadata": {},
   "source": [
    "### Fuzzy C-Means Clustering\n",
    "\n",
    "The [fuzzy clustering](https://en.wikipedia.org/wiki/Fuzzy_clustering)\n",
    "version of the k-means clustering problem. Each data point has a degree\n",
    "of membership to for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"e1071\")\n",
    "\n",
    "cluster_cmeans <- cmeans(as.matrix(ruspini_scaled), centers = 4)\n",
    "cluster_cmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4c0d1",
   "metadata": {},
   "source": [
    "Plot membership (shown as small pie charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75547289",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"scatterpie\")\n",
    "ggplot()  +\n",
    "  geom_scatterpie(data = cbind(ruspini_scaled, cluster_cmeans$membership),\n",
    "    aes(x = x, y = y), cols = colnames(cluster_cmeans$membership), legend_name = \"Membership\") + coord_equal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a7119",
   "metadata": {},
   "source": [
    "## Internal Cluster Validation\n",
    "\n",
    "### Compare the Clustering Quality\n",
    "\n",
    "The two most popular quality metrics are the within-cluster sum of\n",
    "squares (WCSS) used by\n",
    "[$k$-means](https://en.wikipedia.org/wiki/K-means_clustering) and the\n",
    "[average silhouette\n",
    "width](https://en.wikipedia.org/wiki/Silhouette_(clustering)). Look at\n",
    "`within.cluster.ss` and `avg.silwidth` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##library(fpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd871e0f",
   "metadata": {},
   "source": [
    "Notes: \\* I do not load fpc since the NAMESPACE overwrites dbscan. \\*\n",
    "The clustering (second argument below) has to be supplied as a vector\n",
    "with numbers (cluster IDs) and cannot be a factor (use `as.integer()` to\n",
    "convert the factor to an ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48196214",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpc::cluster.stats(d, km$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31b99e",
   "metadata": {},
   "source": [
    "Read `? cluster.stats` for an explanation of all the available indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c27c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(\n",
    "  list(\n",
    "    km = km$cluster,\n",
    "    hc_compl = cutree(hc, k = 4),\n",
    "    hc_single = cutree(hc_single, k = 4)\n",
    "  ),\n",
    "  FUN = function(x)\n",
    "    fpc::cluster.stats(d, x))[c(\"within.cluster.ss\", \"avg.silwidth\"), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca586a",
   "metadata": {},
   "source": [
    "### Silhouette plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(cluster)\n",
    "plot(silhouette(km$cluster, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f040384",
   "metadata": {},
   "source": [
    "**Note:** The silhouette plot does not show correctly in R Studio if you\n",
    "have too many objects (bars are missing). I will work when you open a\n",
    "new plotting device with `windows()`, `x11()` or `quartz()`.\n",
    "\n",
    "ggplot visualization using `factoextra`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_silhouette(silhouette(km$cluster, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db194b6",
   "metadata": {},
   "source": [
    "### Find Optimal Number of Clusters for k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa845de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(ruspini_scaled, aes(x, y)) + geom_point()\n",
    "\n",
    "## We will use different methods and try 1-10 clusters.\n",
    "set.seed(1234)\n",
    "ks <- 2:10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad19299",
   "metadata": {},
   "source": [
    "#### Elbow Method: Within-Cluster Sum of Squares\n",
    "\n",
    "Calculate the within-cluster sum of squares for different numbers of\n",
    "clusters and look for the [knee or\n",
    "elbow](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) in the\n",
    "plot. (`nstart = 5` just repeats k-means 5 times and returns the best\n",
    "solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WCSS <- sapply(ks, FUN = function(k) {\n",
    "  kmeans(ruspini_scaled, centers = k, nstart = 5)$tot.withinss\n",
    "  })\n",
    "\n",
    "ggplot(as_tibble(ks, WCSS), aes(ks, WCSS)) + geom_line() +\n",
    "  geom_vline(xintercept = 4, color = \"red\", linetype = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e349dfc",
   "metadata": {},
   "source": [
    "#### Average Silhouette Width\n",
    "\n",
    "Plot the average silhouette width for different number of clusters and\n",
    "look for the maximum in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASW <- sapply(ks, FUN=function(k) {\n",
    "  fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart = 5)$cluster)$avg.silwidth\n",
    "  })\n",
    "\n",
    "best_k <- ks[which.max(ASW)]\n",
    "best_k\n",
    "\n",
    "ggplot(as_tibble(ks, ASW), aes(ks, ASW)) + geom_line() +\n",
    "  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea4841",
   "metadata": {},
   "source": [
    "#### Dunn Index\n",
    "\n",
    "Use [Dunn index](https://en.wikipedia.org/wiki/Dunn_index) (another\n",
    "internal measure given by min. separation/ max. diameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa06e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "DI <- sapply(ks, FUN=function(k) {\n",
    "  fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart=5)$cluster)$dunn\n",
    "})\n",
    "\n",
    "best_k <- ks[which.max(DI)]\n",
    "ggplot(as_tibble(ks, DI), aes(ks, DI)) + geom_line() +\n",
    "  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7dbd6",
   "metadata": {},
   "source": [
    "#### Gap Statistic\n",
    "\n",
    "Compares the change in within-cluster dispersion with that expected from\n",
    "a null model (see `? clusGap`). The default method is to choose the\n",
    "smallest k such that its value Gap(k) is not more than 1 standard error\n",
    "away from the first local maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(cluster)\n",
    "k <- clusGap(ruspini_scaled, FUN = kmeans,  nstart = 10, K.max = 10)\n",
    "k\n",
    "plot(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6854a87",
   "metadata": {},
   "source": [
    "**Note:** these methods can also be used for hierarchical clustering.\n",
    "\n",
    "There have been many other methods and indices proposed to determine the\n",
    "number of clusters. See, e.g., package\n",
    "[NbClust](https://cran.r-project.org/package=NbClust).\n",
    "\n",
    "### Visualizing the Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(ruspini_scaled, aes(x, y, color = factor(km$cluster))) + geom_point()\n",
    "\n",
    "d <- dist(ruspini_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fe400",
   "metadata": {},
   "source": [
    "Inspect the distance matrix between the first 5 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "as.matrix(d)[1:5, 1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a70ce8",
   "metadata": {},
   "source": [
    "A false-color image visualizes each value in the matrix as a pixel with\n",
    "the color representing the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(seriation)\n",
    "pimage(d, col = bluered(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac61e7",
   "metadata": {},
   "source": [
    "Rows and columns are the objects as they are ordered in the data set.\n",
    "The diagonal represents the distance between an object and itself and\n",
    "has by definition a distance of 0 (dark line). Visualizing the unordered\n",
    "distance matrix does not show much structure, but we can reorder the\n",
    "matrix (rows and columns) using the k-means cluster labels from cluster\n",
    "1 to 4. A clear block structure representing the clusters becomes\n",
    "visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pimage(d, order=order(km$cluster), col = bluered(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863ec8c",
   "metadata": {},
   "source": [
    "Plot function `dissplot` in package **seriation** rearranges the matrix\n",
    "and adds lines and cluster labels. In the lower half of the plot, it\n",
    "shows average dissimilarities between clusters. The function organizes\n",
    "the objects by cluster and then reorders clusters and objects within\n",
    "clusters so that more similar objects are closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissplot(d, labels = km$cluster, options=list(main=\"k-means with k=4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64caf102",
   "metadata": {},
   "source": [
    "The reordering by `dissplot` makes the misspecification of k visible as\n",
    "blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b2c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissplot(d, labels = kmeans(ruspini_scaled, centers = 3)$cluster, col = bluered(100))\n",
    "dissplot(d, labels = kmeans(ruspini_scaled, centers = 9)$cluster, col = bluered(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc75080",
   "metadata": {},
   "source": [
    "Using `factoextra`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_dist(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1ccf5",
   "metadata": {},
   "source": [
    "## External Cluster Validation\n",
    "\n",
    "External cluster validation uses ground truth information. That is, the\n",
    "user has an idea how the data should be grouped. This could be a known\n",
    "class label not provided to the clustering algorithm.\n",
    "\n",
    "We use an artificial data set with known groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(mlbench)\n",
    "set.seed(1234)\n",
    "shapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)\n",
    "plot(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde384b0",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth <- as.integer(shapes$class)\n",
    "shapes <- scale(shapes$x)\n",
    "colnames(shapes) <- c(\"x\", \"y\")\n",
    "shapes <- as_tibble(shapes)\n",
    "\n",
    "ggplot(shapes, aes(x, y)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5b5ce",
   "metadata": {},
   "source": [
    "Find optimal number of Clusters for k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ded6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks <- 2:20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e5356",
   "metadata": {},
   "source": [
    "Use within sum of squares (look for the knee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "WCSS <- sapply(ks, FUN = function(k) {\n",
    "  kmeans(shapes, centers = k, nstart = 10)$tot.withinss\n",
    "})\n",
    "\n",
    "ggplot(as_tibble(ks, WCSS), aes(ks, WCSS)) + geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3845f6",
   "metadata": {},
   "source": [
    "Looks like it could be 7 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bdf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "km <- kmeans(shapes, centers = 7, nstart = 10)\n",
    "\n",
    "ggplot(shapes %>% add_column(cluster = factor(km$cluster)), aes(x, y, color = cluster)) +\n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8af85",
   "metadata": {},
   "source": [
    "Hierarchical clustering: We use single-link because of the mouth is\n",
    "non-convex and chaining may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798da1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d <- dist(shapes)\n",
    "hc <- hclust(d, method = \"single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25034dc1",
   "metadata": {},
   "source": [
    "Find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASW <- sapply(ks, FUN = function(k) {\n",
    "  fpc::cluster.stats(d, cutree(hc, k))$avg.silwidth\n",
    "})\n",
    "\n",
    "ggplot(as_tibble(ks, ASW), aes(ks, ASW)) + geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6fe781",
   "metadata": {},
   "source": [
    "The maximum is clearly at 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_4 <- cutree(hc, 4)\n",
    "\n",
    "ggplot(shapes %>% add_column(cluster = factor(hc_4)), aes(x, y, color = cluster)) +\n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f80408c",
   "metadata": {},
   "source": [
    "Compare with ground truth with the [corrected (=adjusted) Rand index\n",
    "(ARI)](https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index),\n",
    "the [variation of information (VI)\n",
    "index](https://en.wikipedia.org/wiki/Variation_of_information),\n",
    "[entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "and\n",
    "[purity](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation).\n",
    "\n",
    "`cluster_stats` computes ARI and VI as comparative measures. I define\n",
    "functions for entropy and purity here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy <- function(cluster, truth) {\n",
    "  k <- max(cluster, truth)\n",
    "  cluster <- factor(cluster, levels = 1:k)\n",
    "  truth <- factor(truth, levels = 1:k)\n",
    "  w <- table(cluster)/length(cluster)\n",
    "\n",
    "  cnts <- sapply(split(truth, cluster), table)\n",
    "  p <- sweep(cnts, 1, rowSums(cnts), \"/\")\n",
    "  p[is.nan(p)] <- 0\n",
    "  e <- -p * log(p, 2)\n",
    "\n",
    "  sum(w * rowSums(e, na.rm = TRUE))\n",
    "}\n",
    "\n",
    "purity <- function(cluster, truth) {\n",
    "  k <- max(cluster, truth)\n",
    "  cluster <- factor(cluster, levels = 1:k)\n",
    "  truth <- factor(truth, levels = 1:k)\n",
    "  w <- table(cluster)/length(cluster)\n",
    "\n",
    "  cnts <- sapply(split(truth, cluster), table)\n",
    "  p <- sweep(cnts, 1, rowSums(cnts), \"/\")\n",
    "  p[is.nan(p)] <- 0\n",
    "\n",
    "  sum(w * apply(p, 1, max))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be5941",
   "metadata": {},
   "source": [
    "calculate measures (for comparison we also use random \"clusterings\" with\n",
    "4 and 6 clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_4 <- sample(1:4, nrow(shapes), replace = TRUE)\n",
    "random_6 <- sample(1:6, nrow(shapes), replace = TRUE)\n",
    "\n",
    "r <- rbind(\n",
    "  kmeans_7 = c(\n",
    "    unlist(fpc::cluster.stats(d, km$cluster, truth, compareonly = TRUE)),\n",
    "    entropy = entropy(km$cluster, truth),\n",
    "    purity = purity(km$cluster, truth)\n",
    "    ),\n",
    "  hc_4 = c(\n",
    "    unlist(fpc::cluster.stats(d, hc_4, truth, compareonly = TRUE)),\n",
    "    entropy = entropy(hc_4, truth),\n",
    "    purity = purity(hc_4, truth)\n",
    "    ),\n",
    "  random_4 = c(\n",
    "    unlist(fpc::cluster.stats(d, random_4, truth, compareonly = TRUE)),\n",
    "    entropy = entropy(random_4, truth),\n",
    "    purity = purity(random_4, truth)\n",
    "    ),\n",
    "  random_6 = c(\n",
    "    unlist(fpc::cluster.stats(d, random_6, truth, compareonly = TRUE)),\n",
    "    entropy = entropy(random_6, truth),\n",
    "    purity = purity(random_6, truth)\n",
    "    )\n",
    "  )\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b742b3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "-   Hierarchical clustering found the perfect clustering.\n",
    "-   Entropy and purity are heavily impacted by the number of clusters\n",
    "    (more clusters improve the metric).\n",
    "-   The corrected rand index shows clearly that the random clusterings\n",
    "    have no relationship with the ground truth (very close to 0). This\n",
    "    is a very helpful property.\n",
    "\n",
    "Read `? cluster.stats` for an explanation of all the available indices.\n",
    "\n",
    "## Advanced Data Preparation for Clustering\n",
    "\n",
    "### Outlier Removal\n",
    "\n",
    "Most clustering algorithms perform complete assignment (i.e., all data\n",
    "points need to be assigned to a cluster). Outliers will affect the\n",
    "clustering. It is useful to identify outliers and remove strong outliers\n",
    "prior to clustering. A density based method to identify outlier is\n",
    "[LOF](https://en.wikipedia.org/wiki/Local_outlier_factor) (Local Outlier\n",
    "Factor). It is related to dbscan and compares the density around a point\n",
    "with the densities around its neighbors (you have to specify the\n",
    "neighborhood size $k$). The LOF value for a regular data point is 1. The\n",
    "larger the LOF value gets, the more likely the point is an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef158096",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13cbe45",
   "metadata": {},
   "source": [
    "Add a clear outlier to the scaled Ruspini dataset that is 10 standard\n",
    "deviations above the average for the x axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b70b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruspini_scaled_outlier <- ruspini_scaled %>% add_case(x=10,y=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52467611",
   "metadata": {},
   "source": [
    "#### Visual inspection of the data\n",
    "\n",
    "Outliers can be identified using summary statistics, histograms,\n",
    "scatterplots (pairs plots), and boxplots, etc. We use here a pairs plot\n",
    "(the diagonal contains smoothed histograms). The outlier is visible as\n",
    "the single separate point in the scatter plot and as the long tail of\n",
    "the smoothed histogram for `x` (we would expect most observations to\n",
    "fall in the range \\[-3,3\\] in normalized data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcad8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"GGally\")\n",
    "ggpairs(ruspini_scaled_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591be61",
   "metadata": {},
   "source": [
    "The outlier is a problem for k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a5566",
   "metadata": {},
   "outputs": [],
   "source": [
    "km <- kmeans(ruspini_scaled_outlier, centers = 4, nstart = 10)\n",
    "ruspini_scaled_outlier_km <- ruspini_scaled_outlier%>%\n",
    "  add_column(cluster = factor(km$cluster))\n",
    "centroids <- as_tibble(km$centers, rownames = \"cluster\")\n",
    "\n",
    "ggplot(ruspini_scaled_outlier_km, aes(x = x, y = y, color = cluster)) + geom_point() +\n",
    "  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189043ea",
   "metadata": {},
   "source": [
    "This problem can be fixed by increasing the number of clusters and\n",
    "removing small clusters in a post-processing step or by identifying and\n",
    "removing outliers before clustering.\n",
    "\n",
    "#### Local Outlier Factor (LOF)\n",
    "\n",
    "The [Local Outlier\n",
    "Factor](https://en.wikipedia.org/wiki/Local_outlier_factor) is related\n",
    "to concepts of DBSCAN can help to identify potential outliers. Calculate\n",
    "the LOF (I choose a local neighborhood size of 10 for density estimation),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3995fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof <- lof(ruspini_scaled_outlier, minPts= 10)\n",
    "lof\n",
    "\n",
    "ggplot(ruspini_scaled_outlier %>% add_column(lof = lof), aes(x, y, color = lof)) +\n",
    "    geom_point() + scale_color_gradient(low = \"gray\", high = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce06375",
   "metadata": {},
   "source": [
    "Plot the points sorted by increasing LOF and look for a knee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0925cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(tibble(index = seq_len(length(lof)), lof = sort(lof)), aes(index, lof)) +\n",
    "  geom_line() +\n",
    "  geom_hline(yintercept = 1, color = \"red\", linetype = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bed92",
   "metadata": {},
   "source": [
    "Choose a threshold above 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(ruspini_scaled_outlier %>% add_column(outlier = lof >= 2), aes(x, y, color = outlier)) +\n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a9a7f",
   "metadata": {},
   "source": [
    "Analyze the found outliers (they might be interesting data points) and then cluster the data without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a55c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruspini_scaled_clean <- ruspini_scaled_outlier  %>% filter(lof < 2)\n",
    "\n",
    "km <- kmeans(ruspini_scaled_clean, centers = 4, nstart = 10)\n",
    "ruspini_scaled_clean_km <- ruspini_scaled_clean%>%\n",
    "  add_column(cluster = factor(km$cluster))\n",
    "centroids <- as_tibble(km$centers, rownames = \"cluster\")\n",
    "\n",
    "ggplot(ruspini_scaled_clean_km, aes(x = x, y = y, color = cluster)) + geom_point() +\n",
    "  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa167b",
   "metadata": {},
   "source": [
    "There are many other outlier removal strategies available. See, e.g.,\n",
    "package [outliers](https://cran.r-project.org/package=outliers).\n",
    "\n",
    "### Clustering Tendency\n",
    "\n",
    "Most clustering algorithms will always produce a clustering, even if the\n",
    "data does not contain a cluster structure. It is typically good to check\n",
    "cluster tendency before attempting to cluster the data.\n",
    "\n",
    "We use again the smiley data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82954368",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(mlbench)\n",
    "shapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)$x\n",
    "colnames(shapes) <- c(\"x\", \"y\")\n",
    "shapes <- as_tibble(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ef674",
   "metadata": {},
   "source": [
    "#### Scatter plots\n",
    "\n",
    "The first step is visual inspection using scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf62a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(shapes, aes(x = x, y = y)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290bf066",
   "metadata": {},
   "source": [
    "Cluster tendency is typically indicated by several separated point\n",
    "clouds. Often an appropriate number of clusters can also be visually\n",
    "obtained by counting the number of point clouds. We see four clusters,\n",
    "but the mouth is not convex/spherical and thus will pose a problems to\n",
    "algorithms like k-means.\n",
    "\n",
    "If the data has more than two features then you can use a pairs plot\n",
    "(scatterplot matrix) or look at a scatterplot of the first two principal\n",
    "components using PCA. \n",
    "\n",
    "#### Visual Analysis for Cluster Tendency Assessment (VAT)\n",
    "\n",
    "VAT reorders the objects to show potential clustering tendency as a\n",
    "block structure (dark blocks along the main diagonal). We scale the data\n",
    "before using Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a924fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(seriation)\n",
    "\n",
    "d_shapes <- dist(scale(shapes))\n",
    "VAT(d_shapes, col = bluered(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ae84a",
   "metadata": {},
   "source": [
    "iVAT uses the largest distances for all possible paths between two\n",
    "objects instead of the direct distances to make the block structure\n",
    "better visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde28088",
   "metadata": {},
   "outputs": [],
   "source": [
    "iVAT(d_shapes, col = bluered(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef93d0",
   "metadata": {},
   "source": [
    "#### Hopkins statistic\n",
    "\n",
    "`factoextra` can also create a VAT plot and calculate the [Hopkins\n",
    "statistic](https://en.wikipedia.org/wiki/Hopkins_statistic) to assess\n",
    "clustering tendency. For the Hopkins statistic, a sample of size $n$ is\n",
    "drawn from the data and then compares the nearest neighbor distribution\n",
    "with a simulated dataset drawn from a random uniform distribution (see\n",
    "[detailed\n",
    "explanation](https://www.datanovia.com/en/lessons/assessing-clustering-tendency/#statistical-methods)).\n",
    "A values \\>.5 indicates usually a clustering tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clust_tendency(shapes, n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e715b",
   "metadata": {},
   "source": [
    "Both plots show a strong cluster structure with 4 clusters.\n",
    "\n",
    "#### Data Without Clustering Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_random <- tibble(x = runif(500), y = runif(500))\n",
    "ggplot(data_random, aes(x, y)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf51f2f",
   "metadata": {},
   "source": [
    "No point clouds are visible, just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ef802",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_random <- dist(data_random)\n",
    "VAT(d_random, col = bluered(100))\n",
    "iVAT(d_random, col = bluered(100))\n",
    "get_clust_tendency(data_random, n = 10, graph = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dac274",
   "metadata": {},
   "source": [
    "There is very little clustering structure visible indicating low\n",
    "clustering tendency and clustering should not be performed on this data.\n",
    "However, k-means can be used to partition the data into $k$ regions of\n",
    "roughly equivalent size. This can be used as a data-driven\n",
    "discretization of the space.\n",
    "\n",
    "#### k-means on Data Without Clustering Tendency\n",
    "\n",
    "What happens if we perform k-means on data that has no inherent\n",
    "clustering structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ba8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "km <- kmeans(data_random, centers = 4)\n",
    "random_clustered<- data_random %>% add_column(cluster = factor(km$cluster))\n",
    "ggplot(random_clustered, aes(x = x, y = y, color = cluster)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc323770",
   "metadata": {},
   "source": [
    "k-means discretizes the space into similarly sized regions."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "name,tags,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
