{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a4ee6d5f",
   "metadata": {},
   "source": [
    "---\n",
    "editor_options: \n",
    "  markdown: \n",
    "    wrap: 72\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2162a",
   "metadata": {},
   "source": [
    "# Classification: Alternative Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce26687",
   "metadata": {
    "name": "setup_04",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pkgs <- sort(c(\n",
    "'tidyverse',\n",
    "'caret',\n",
    "'RWeka',\n",
    "'lattice',\n",
    "'scales',\n",
    "'e1071',\n",
    "'MASS',\n",
    "'nnet',\n",
    "'rpart',\n",
    "'C50',\n",
    "'randomForest',\n",
    "'keras',\n",
    "'mlbench'\n",
    "  ))\n",
    "\n",
    "lapply(pkgs, function(pkg) {\n",
    "  if (system.file(package = pkg) == '') install.packages(pkg)\n",
    "})\n",
    "\n",
    "all_pkgs <- union(all_pkgs, pkgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd84011",
   "metadata": {},
   "source": [
    "**Packages used for this chapter:** `r format_pkgs(pkgs)`\n",
    "\n",
    "We will use tidyverse to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5436daf",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9d524",
   "metadata": {},
   "source": [
    "Show fewer digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ae151",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cff5f",
   "metadata": {},
   "source": [
    "## Training and Test Data\n",
    "\n",
    "We will use the Zoo dataset which is included in the R package `mlbench`\n",
    "(you may have to install it). The Zoo dataset containing 17 (mostly\n",
    "logical) variables on different 101 animals as a data frame with 17\n",
    "columns (hair, feathers, eggs, milk, airborne, aquatic, predator,\n",
    "toothed, backbone, breathes, venomous, fins, legs, tail, domestic,\n",
    "catsize, type). We convert the data frame into a tidyverse tibble\n",
    "(optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(Zoo, package=\"mlbench\")\n",
    "Zoo <- as_tibble(Zoo)\n",
    "Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072402a",
   "metadata": {},
   "source": [
    "We will use the package [**caret**](https://topepo.github.io/caret/) to\n",
    "make preparing training sets and building classification (and\n",
    "regression) models easier. A great cheat sheet can be found\n",
    "[here](https://ugoproto.github.io/ugo_r_doc/pdf/caret.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb7f9e",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "library(caret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550bb8b9",
   "metadata": {},
   "source": [
    "Multi-core support can be used for cross-validation. **Note:** It is\n",
    "commented out here because it does not work with rJava used in RWeka\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b11bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##library(doMC, quietly = TRUE)\n",
    "##registerDoMC(cores = 4)\n",
    "##getDoParWorkers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eccbcb7",
   "metadata": {},
   "source": [
    "Test data is not used in the model building process and needs to be set\n",
    "aside purely for testing the model after it is completely built. Here I\n",
    "use 80% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5154a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inTrain <- createDataPartition(y = Zoo$type, p = .8, list = FALSE)\n",
    "Zoo_train <- Zoo %>% slice(inTrain)\n",
    "Zoo_test <- Zoo %>% slice(-inTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8dc63b",
   "metadata": {},
   "source": [
    "## Fitting Different Classification Models to the Training Data\n",
    "\n",
    "Create a fixed sampling scheme (10-folds) so we can compare the fitted\n",
    "models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index <- createFolds(Zoo_train$type, k = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4312abc1",
   "metadata": {},
   "source": [
    "The fixed folds are used in `train()` with the argument\n",
    "`trControl = trainControl(method = \"cv\", indexOut = train_index))`. If\n",
    "you don't need fixed folds, then remove `indexOut = train_index` in the\n",
    "code below.\n",
    "\n",
    "For help with building models in caret see: `? train`\n",
    "\n",
    "**Note:** Be careful if you have many `NA` values in your data.\n",
    "`train()` and cross-validation many fail in some cases. If that is the\n",
    "case then you can remove features (columns) which have many `NA`s, omit\n",
    "`NA`s using `na.omit()` or use imputation to replace them with\n",
    "reasonable values (e.g., by the feature mean or via kNN). Highly\n",
    "imbalanced datasets are also problematic since there is a chance that a\n",
    "fold does not contain examples of each class leading to a hard to\n",
    "understand error message.\n",
    "\n",
    "### Conditional Inference Tree (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctreeFit <- Zoo_train %>% train(type ~ .,\n",
    "  method = \"ctree\",\n",
    "  data = .,\n",
    "\ttuneLength = 5,\n",
    "\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\n",
    "ctreeFit\n",
    "plot(ctreeFit$finalModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481298a9",
   "metadata": {},
   "source": [
    "### C 4.5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(RWeka)\n",
    "C45Fit <- Zoo_train %>% train(type ~ .,\n",
    "  method = \"J48\",\n",
    "  data = .,\n",
    "\ttuneLength = 5,\n",
    "\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\n",
    "C45Fit\n",
    "C45Fit$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f4b05",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "**Note:** kNN uses Euclidean distance, so data should be standardized\n",
    "(scaled) first. Here legs are measured between 0 and 6 while all other\n",
    "variables are between 0 and 1. Scaling can be directly performed as\n",
    "preprocessing in `train` using the parameter `preProcess = \"scale\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d9003",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnFit <- Zoo_train %>% train(type ~ .,\n",
    "  method = \"knn\",\n",
    "  data = .,\n",
    "  preProcess = \"scale\",\n",
    "\ttuneLength = 5,\n",
    "  tuneGrid=data.frame(k = 1:10),\n",
    "\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\n",
    "knnFit\n",
    "\n",
    "knnFit$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1865e704",
   "metadata": {},
   "source": [
    "### PART (Rule-based classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rulesFit <- Zoo_train %>% train(type ~ .,\n",
    "  method = \"PART\",\n",
    "  data = .,\n",
    "  tuneLength = 5,\n",
    "  trControl = trainControl(method = \"cv\", indexOut = train_index))\n",
    "rulesFit\n",
    "\n",
    "rulesFit$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537a0bb",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b536fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svmFit <- Zoo_train %>% train(type ~.,\n",
    "  method = \"svmLinear\",\n",
    "  data = .,\n",
    "\ttuneLength = 5,\n",
    "\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\n",
    "svmFit\n",
    "\n",
    "svmFit$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c43863",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8927de",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestFit <- Zoo_train %>% train(type ~ .,\n",
    "  method = \"rf\",\n",
    "  data = .,\n",
    "\ttuneLength = 5,\n",
    "\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\n",
    "randomForestFit\n",
    "\n",
    "randomForestFit$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcc1d7",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Trees (xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboostFit <- Zoo_train %>% train(type ~ .,\n",
    "  method = \"xgbTree\",\n",
    "  data = .,\n",
    "  tuneLength = 5,\n",
    "  trControl = trainControl(method = \"cv\", indexOut = train_index),\n",
    "  tuneGrid = expand.grid(\n",
    "    nrounds = 20,\n",
    "    max_depth = 3,\n",
    "    colsample_bytree = .6,\n",
    "    eta = 0.1,\n",
    "    gamma=0,\n",
    "    min_child_weight = 1,\n",
    "    subsample = .5\n",
    "  ))\n",
    "xgboostFit\n",
    "\n",
    "xgboostFit$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dcc913",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnetFit <- Zoo_train %>% train(type ~ .,\n",
    "  method = \"nnet\",\n",
    "  data = .,\n",
    "\ttuneLength = 5,\n",
    "\ttrControl = trainControl(method = \"cv\", indexOut = train_index),\n",
    "  trace = FALSE)\n",
    "nnetFit\n",
    "\n",
    "nnetFit$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a75ec",
   "metadata": {},
   "source": [
    "## Comparing Models\n",
    "\n",
    "Collect the performance metrics from the models trained on the same\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resamps <- resamples(list(\n",
    "  ctree = ctreeFit,\n",
    "  C45 = C45Fit,\n",
    "  SVM = svmFit,\n",
    "  KNN = knnFit,\n",
    "  rules = rulesFit,\n",
    "  randomForest = randomForestFit,\n",
    "  xgboost = xgboostFit,\n",
    "  NeuralNet = nnetFit\n",
    "    ))\n",
    "resamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde757e1",
   "metadata": {},
   "source": [
    "Calculate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b43d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(resamps)\n",
    "\n",
    "library(lattice)\n",
    "bwplot(resamps, layout = c(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da48075",
   "metadata": {},
   "source": [
    "Perform inference about differences between models. For each metric, all\n",
    "pair-wise differences are computed and tested to assess if the\n",
    "difference is equal to zero. By default Bonferroni correction for\n",
    "multiple comparison is used. Differences are shown in the upper triangle\n",
    "and p-values are in the lower triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934607c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "difs <- diff(resamps)\n",
    "difs\n",
    "summary(difs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac45a95",
   "metadata": {},
   "source": [
    "All perform similarly well except ctree (differences in the first row\n",
    "are negative and the p-values in the first column are \\<.05 indicating\n",
    "that the null-hypothesis of a difference of 0 can be rejected).\n",
    "\n",
    "## Applying the Chosen Model to the Test Data\n",
    "\n",
    "Most models do similarly well on the data. We choose here the random\n",
    "forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr <- predict(randomForestFit, Zoo_test)\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7c4c1",
   "metadata": {},
   "source": [
    "Calculate the confusion matrix for the held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix(pr, reference = Zoo_test$type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366460e",
   "metadata": {},
   "source": [
    "## Deep Learning with keras/tensorflow\n",
    "\n",
    "The keras package needs the packages `reticulate` and `tensorflow`. To\n",
    "install keras you need to\n",
    "\n",
    "1.  have a working Python installation,\n",
    "2.  install the keras R package `install.packages(\"keras\")`, and\n",
    "3.  install the tensorflow/keras Python modules with\n",
    "    `library(keras); install_keras()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e07c3",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "library(keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888548f3",
   "metadata": {},
   "source": [
    "Prepare the data. All data needs to be in a matrix of all\n",
    "numeric/integer values. The class variable needs to be one-hot encodes\n",
    "with the `keras` function `to_categorical()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071fd300",
   "metadata": {
    "warning": false
   },
   "outputs": [],
   "source": [
    "X <- Zoo_train %>% select(!type) %>% \n",
    "  mutate(across(everything(), as.integer)) %>% as.matrix()\n",
    "head(X)\n",
    "\n",
    "y <- Zoo_train %>% pull(\"type\") %>% as.integer() %>% `-`(1L) %>% to_categorical()\n",
    "head(y)\n",
    "\n",
    "X <- Zoo_train %>% select(!type) %>% \n",
    "  mutate(across(everything(), as.integer)) %>% as.matrix()\n",
    "head(X)\n",
    "\n",
    "y <- Zoo_train %>% pull(\"type\") %>% as.integer() %>% `-`(1L) %>% to_categorical()\n",
    "head(y)\n",
    "\n",
    "X_test <- Zoo_test %>% select(!type) %>% \n",
    "  mutate(across(everything(), as.integer)) %>% as.matrix()\n",
    "y_test <- Zoo_test %>% pull(\"type\") %>% as.integer() %>% `-`(1L) %>% to_categorical()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13062a6f",
   "metadata": {},
   "source": [
    "Deep learning uses a large set of hyper-parameters. Choices are the\n",
    "activation function, number of layers, number of units per layer and the\n",
    "optimizer. A L2 regularizer is used for the dense layer weights to\n",
    "reduce overfitting. The output is a categorical class value, therefore\n",
    "the output layer uses the softmax activation function, the loss is\n",
    "categorical cross-entropy, and the metric is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2ce94",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "  layer_dense(units = 10, activation = 'relu', input_shape = c(ncol(X)),\n",
    "    kernel_regularizer=regularizer_l2(l=0.01)) %>%\n",
    "  layer_dense(units = ncol(y), activation = 'softmax') %>%\n",
    "  compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac36859",
   "metadata": {},
   "source": [
    "For model training, we need to specify the batch size and the number of\n",
    "training epochs. The fitting process can also use a fraction of the\n",
    "training data for validation to provide generalization loss/accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- model %>% fit(\n",
    "  X, y,\n",
    "  batch_size = 10,\n",
    "  epochs = 100,\n",
    "  validation_split = .2\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6670a0",
   "metadata": {},
   "source": [
    "To create predictions from the model, we have to convert the one-hot\n",
    "encoding back to class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cfacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels <- levels(Zoo_train %>% pull(type))\n",
    "\n",
    "pr <- predict(model, X_test) %>% apply(MARGIN = 1, FUN = which.max)\n",
    "pr <- factor(pr, labels = class_labels, levels = seq_along(class_labels))\n",
    "\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c709e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix(pr, reference = Zoo_test$type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54632f",
   "metadata": {},
   "source": [
    "## Comparing Decision Boundaries of Popular Classification Techniques\n",
    "\n",
    "Classifiers create decision boundaries to discriminate between classes.\n",
    "Different classifiers are able to create different shapes of decision\n",
    "boundaries (e.g., some are strictly linear) and thus some classifiers\n",
    "may perform better for certain datasets. This page visualizes the\n",
    "decision boundaries found by several popular classification methods.\n",
    "\n",
    "The following plot adds the decision boundary (black lines) and\n",
    "classification confidence (color intensity) by evaluating the classifier\n",
    "at evenly spaced grid points. Note that low resolution (to make\n",
    "evaluation faster) will make the decision boundary look like it has\n",
    "small steps even if it is a (straight) line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e77585",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "library(scales)\n",
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(caret)\n",
    "\n",
    "decisionplot <- function(model, data, class_var, \n",
    "  predict_type = c(\"class\", \"prob\"), resolution = 5 * 75) {\n",
    "  # resolution is set to 75 dpi if the image is rendered  5 inces wide. \n",
    "  \n",
    "  y <- data %>% pull(class_var)\n",
    "  x <- data %>% dplyr::select(-all_of(class_var))\n",
    "  \n",
    "  # resubstitution accuracy\n",
    "  prediction <- predict(model, x, type = predict_type[1])\n",
    "  # LDA returns a list\n",
    "  if(is.list(prediction)) prediction <- prediction$class\n",
    "  prediction <- factor(prediction, levels = levels(y))\n",
    "  \n",
    "  cm <- confusionMatrix(data = prediction, reference = y)\n",
    "  acc <- cm$overall[\"Accuracy\"]\n",
    "  \n",
    "  # evaluate model on a grid\n",
    "  r <- sapply(x[, 1:2], range, na.rm = TRUE)\n",
    "  xs <- seq(r[1,1], r[2,1], length.out = resolution)\n",
    "  ys <- seq(r[1,2], r[2,2], length.out = resolution)\n",
    "  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))\n",
    "  colnames(g) <- colnames(r)\n",
    "  g <- as_tibble(g)\n",
    "  \n",
    "  ### guess how to get class labels from predict\n",
    "  ### (unfortunately not very consistent between models)\n",
    "  cl <- predict(model, g, type = predict_type[1])\n",
    "  \n",
    "  # LDA returns a list\n",
    "  if(is.list(cl)) { \n",
    "    prob <- cl$posterior\n",
    "    cl <- cl$class\n",
    "  } else\n",
    "    try(prob <- predict(model, g, type = predict_type[2]))\n",
    "  \n",
    "  # we visualize the difference in probability/score between the \n",
    "  # winning class and the second best class.\n",
    "  # don't use probability if predict for the classifier does not support it.\n",
    "  max_prob <- 1\n",
    "  try({\n",
    "    max_prob <- t(apply(prob, MARGIN = 1, sort, decreasing = TRUE))\n",
    "    max_prob <- max_prob[,1] - max_prob[,2]\n",
    "  }, silent = TRUE) \n",
    "  \n",
    "  cl <- factor(cl, levels = levels(y))\n",
    "  \n",
    "  g <- g %>% add_column(prediction = cl, probability = max_prob)\n",
    "  \n",
    "  ggplot(g, mapping = aes_string(\n",
    "    x = colnames(g)[1],\n",
    "    y = colnames(g)[2])) +\n",
    "    geom_raster(mapping = aes(fill = prediction, alpha = probability)) +\n",
    "     geom_contour(mapping = aes(z = as.numeric(prediction)), \n",
    "      bins = length(levels(cl)), size = .5, color = \"black\") +\n",
    "    geom_point(data = data, mapping =  aes_string(\n",
    "      x = colnames(data)[1],\n",
    "      y = colnames(data)[2],\n",
    "      shape = class_var), alpha = .7) + \n",
    "    scale_alpha_continuous(range = c(0,1), limits = c(0,1), guide = \"none\") +  \n",
    "    labs(subtitle = paste(\"Training accuracy:\", round(acc, 2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d598bc",
   "metadata": {},
   "source": [
    "### Iris Dataset\n",
    "\n",
    "For easier visualization, we use two dimensions of the Iris dataset.\n",
    "Contour lines visualize the density like mountains on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6724b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1000)\n",
    "data(iris)\n",
    "iris <- as_tibble(iris)\n",
    "\n",
    "### Three classes (MASS also has a select function)\n",
    "x <- iris %>% dplyr::select(Sepal.Length, Sepal.Width, Species)\n",
    "x\n",
    "\n",
    "ggplot(x, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +  \n",
    "  stat_density_2d(alpha = .2, geom = \"polygon\") +\n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ab880",
   "metadata": {},
   "source": [
    "*Note:* There is some overplotting and you could use `geom_jitter()`\n",
    "instead of `geom_point()`.\n",
    "\n",
    "#### K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)\n",
    "model <- x %>% knn3(Species ~ ., data = ., k = 1)\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"kNN (1 neighbor)\")\n",
    "\n",
    "model <- x %>% knn3(Species ~ ., data = ., k = 3)\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"kNN (3 neighbor)\")\n",
    "\n",
    "model <- x %>% knn3(Species ~ ., data = ., k = 9)\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"kNN (9 neighbor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d97468",
   "metadata": {},
   "source": [
    "Increasing $k$ smooths the decision boundary. At $k=1$, we see white\n",
    "areas around points where flowers of two classes are in the same spot.\n",
    "Here, the algorithm randomly chooses a class during prediction resulting\n",
    "in the meandering decision boundary. The predictions in that area are\n",
    "not stable and every time we ask for a class, we may get a different\n",
    "class.\n",
    "\n",
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(e1071)\n",
    "model <- x %>% naiveBayes(Species ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"Species\", predict_type = c(\"class\", \"raw\")) + labs(title = \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec9dd2",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bcc1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "model <- x %>% lda(Species ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6892e",
   "metadata": {},
   "source": [
    "#### Multinomial Logistic Regression (implemented in nnet)\n",
    "\n",
    "Multinomial logistic regression is an extension of logistic regression\n",
    "to problems with more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27151a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(nnet)\n",
    "model <- x %>% multinom(Species ~., data = .)\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(titel = \"Multinomial Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ff8d6",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"rpart\")\n",
    "model <- x %>% rpart(Species ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"CART\")\n",
    "\n",
    "model <- x %>% rpart(Species ~ ., data = .,\n",
    "  control = rpart.control(cp = 0.001, minsplit = 1))\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"CART (overfitting)\")\n",
    "\n",
    "library(C50)\n",
    "model <- x %>% C5.0(Species ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"C5.0\")\n",
    "\n",
    "library(randomForest)\n",
    "model <- x %>% randomForest(Species ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941362a3",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(e1071)\n",
    "model <- x %>% svm(Species ~ ., data = ., kernel = \"linear\")\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"SVM (linear kernel)\")\n",
    "\n",
    "model <- x %>% svm(Species ~ ., data = ., kernel = \"radial\")\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"SVM (radial kernel)\")\n",
    "\n",
    "model <- x %>% svm(Species ~ ., data = ., kernel = \"polynomial\")\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"SVM (polynomial kernel)\")\n",
    "\n",
    "model <- x %>% svm(Species ~ ., data = ., kernel = \"sigmoid\")\n",
    "decisionplot(model, x, class_var = \"Species\") + labs(title = \"SVM (sigmoid kernel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebc5d1",
   "metadata": {},
   "source": [
    "#### Single Layer Feed-forward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c112c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(nnet)\n",
    "model <-x %>% nnet(Species ~ ., data = ., size = 1, maxit = 1000, trace = FALSE)\n",
    "decisionplot(model, x, class_var  = \"Species\", \n",
    "  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (1 neuron)\")\n",
    "\n",
    "model <-x %>% nnet(Species ~ ., data = ., size = 2, maxit = 1000, trace = FALSE)\n",
    "decisionplot(model, x, class_var  = \"Species\", \n",
    "  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (2 neurons)\")\n",
    "\n",
    "model <-x %>% nnet(Species ~ ., data = ., size = 4, maxit = 1000, trace = FALSE)\n",
    "decisionplot(model, x, class_var  = \"Species\", \n",
    "  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (4 neurons)\")\n",
    "\n",
    "model <-x %>% nnet(Species ~ ., data = ., size = 10, maxit = 1000, trace = FALSE)\n",
    "decisionplot(model, x, class_var  = \"Species\", \n",
    "  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (10 neurons)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b788d",
   "metadata": {},
   "source": [
    "### Circle Dataset\n",
    "\n",
    "This set is not linearly separable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd032ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1000)\n",
    "\n",
    "library(mlbench)\n",
    "x <- mlbench.circle(500)\n",
    "###x <- mlbench.cassini(500)\n",
    "###x <- mlbench.spirals(500, sd = .1)\n",
    "###x <- mlbench.smiley(500)\n",
    "x <- cbind(as.data.frame(x$x), factor(x$classes))\n",
    "colnames(x) <- c(\"x\", \"y\", \"class\")\n",
    "x <- as_tibble(x)\n",
    "x\n",
    "\n",
    "ggplot(x, aes(x = x, y = y, color = class)) + \n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258fdff",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4007d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)\n",
    "model <- x %>% knn3(class ~ ., data = ., k = 1)\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"kNN (1 neighbor)\")\n",
    "\n",
    "model <- x %>% knn3(class ~ ., data = ., k = 10)\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"kNN (10 neighbor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec806d",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(e1071)\n",
    "model <- x %>% naiveBayes(class ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"class\", \n",
    "  predict_type = c(\"class\", \"raw\")) + labs(title = \"naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fe8e7",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis\n",
    "\n",
    "LDA cannot find a good model since the true decision boundary is not\n",
    "linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd31de",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "model <- x %>% lda(class ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ce4ef",
   "metadata": {},
   "source": [
    "#### Multinomial Logistic Regression (implemented in nnet)\n",
    "\n",
    "Multinomial logistic regression is an extension of logistic regression\n",
    "to problems with more than two classes. It also tries to find a linear\n",
    "decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e47327",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(nnet)\n",
    "model <- x %>% multinom(class ~., data = .)\n",
    "decisionplot(model, x, class_var = \"class\") + labs(titel = \"Multinomial Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec7c2a",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae085b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"rpart\")\n",
    "model <- x %>% rpart(class ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"CART\")\n",
    "\n",
    "model <- x %>% rpart(class ~ ., data = .,\n",
    "  control = rpart.control(cp = 0.001, minsplit = 1))\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"CART (overfitting)\")\n",
    "\n",
    "library(C50)\n",
    "model <- x %>% C5.0(class ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"C5.0\")\n",
    "\n",
    "library(randomForest)\n",
    "model <- x %>% randomForest(class ~ ., data = .)\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c87bc3",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "\n",
    "Linear SVM does not work for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(e1071)\n",
    "model <- x %>% svm(class ~ ., data = ., kernel = \"linear\")\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"SVM (linear kernel)\")\n",
    "\n",
    "model <- x %>% svm(class ~ ., data = ., kernel = \"radial\")\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"SVM (radial kernel)\")\n",
    "\n",
    "model <- x %>% svm(class ~ ., data = ., kernel = \"polynomial\")\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"SVM (polynomial kernel)\")\n",
    "\n",
    "model <- x %>% svm(class ~ ., data = ., kernel = \"sigmoid\")\n",
    "decisionplot(model, x, class_var = \"class\") + labs(title = \"SVM (sigmoid kernel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba99e1",
   "metadata": {},
   "source": [
    "#### Single Layer Feed-forward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32163656",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(nnet)\n",
    "model <-x %>% nnet(class ~ ., data = ., size = 1, maxit = 1000, trace = FALSE)\n",
    "decisionplot(model, x, class_var = \"class\", \n",
    "  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (1 neuron)\")\n",
    "\n",
    "model <-x %>% nnet(class ~ ., data = ., size = 2, maxit = 1000, trace = FALSE)\n",
    "decisionplot(model, x, class_var = \"class\", \n",
    "  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (2 neurons)\")\n",
    "\n",
    "model <-x %>% nnet(class ~ ., data = ., size = 4, maxit = 1000, trace = FALSE)\n",
    "decisionplot(model, x, class_var = \"class\", \n",
    "  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (4 neurons)\")\n",
    "\n",
    "model <-x %>% nnet(class ~ ., data = ., size = 10, maxit = 1000, trace = FALSE)\n",
    "decisionplot(model, x, class_var = \"class\", \n",
    "  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (10 neurons)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6c28b",
   "metadata": {},
   "source": [
    "## More Information on Classification with R\n",
    "\n",
    "-   Package caret: <http://topepo.github.io/caret/index.html>\n",
    "-   Tidymodels (machine learning with tidyverse):\n",
    "    <https://www.tidymodels.org/>\n",
    "-   R taskview on machine learning:\n",
    "    <http://cran.r-project.org/web/views/MachineLearning.html>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "warning,tags,name,message,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
