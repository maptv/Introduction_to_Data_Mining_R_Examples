{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1476537c",
   "metadata": {},
   "source": [
    "---\n",
    "editor_options: \n",
    "  markdown: \n",
    "    wrap: 72\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b4317",
   "metadata": {},
   "source": [
    "# Classification: Basic Concepts and Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05cc9b2",
   "metadata": {
    "name": "setup_03",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pkgs <- sort(c('tidyverse', 'rpart', 'rpart.plot', 'caret', \n",
    "  'lattice', 'FSelector', 'sampling', 'pROC', 'mlbench'))\n",
    "\n",
    "lapply(pkgs, function(pkg) {\n",
    "  if (system.file(package = pkg) == '') install.packages(pkg)\n",
    "})\n",
    "\n",
    "all_pkgs <- union(all_pkgs, pkgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed971f1",
   "metadata": {},
   "source": [
    "**Packages used for this chapter:** `r format_pkgs(pkgs)`\n",
    "\n",
    "You can read the free sample chapter from the textbook [@Tan2005]:\n",
    "[Chapter 3. Classification: Basic Concepts and\n",
    "Techniques](https://www-users.cs.umn.edu/~kumar001/dmbook/ch3_classification.pdf)\n",
    "\n",
    "## The Zoo Dataset\n",
    "\n",
    "We will use the Zoo dataset which is included in the R package\n",
    "**mlbench** (you may have to install it). The Zoo dataset containing 17\n",
    "(mostly logical) variables on different 101 animals as a data frame with\n",
    "17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator,\n",
    "toothed, backbone, breathes, venomous, fins, legs, tail, domestic,\n",
    "catsize, type). We convert the data frame into a tidyverse tibble\n",
    "(optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(Zoo, package=\"mlbench\")\n",
    "head(Zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d4ae3",
   "metadata": {},
   "source": [
    "*Note:* data.frames in R can have row names. The Zoo data set uses the\n",
    "animal name as the row names. tibbles from `tidyverse` do not support\n",
    "row names. To keep the animal name you can add a column with the animal\n",
    "name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "as_tibble(Zoo, rownames = \"animal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f208281",
   "metadata": {},
   "source": [
    "You will have to remove the animal column before learning a model! In\n",
    "the following I use the data.frame.\n",
    "\n",
    "I translate all the TRUE/FALSE values into factors (nominal). This is\n",
    "often needed for building models. Always check `summary()` to make sure\n",
    "the data is ready for model learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c93cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zoo <- Zoo %>%\n",
    "  modify_if(is.logical, factor, levels = c(TRUE, FALSE)) %>%\n",
    "  modify_if(is.character, factor)\n",
    "\n",
    "summary(Zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045568c",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Recursive Partitioning (similar to CART) uses the Gini index to make\n",
    "splitting decisions and early stopping (pre-pruning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691958f",
   "metadata": {},
   "source": [
    "### Create Tree With Default Settings (uses pre-pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92557cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_default <- Zoo %>% rpart(type ~ ., data = .)\n",
    "tree_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867de13",
   "metadata": {},
   "source": [
    "**Notes:** - `%>%` supplies the data for `rpart`. Since `data` is not\n",
    "the first argument of `rpart`, the syntax `data = .` is used to specify\n",
    "where the data in `Zoo` goes. The call is equivalent to\n",
    "`tree_default <- rpart(type ~ ., data = Zoo)`. - The formula models the\n",
    "`type` variable by all other features represented by `.`. `data = .`\n",
    "means that the data provided by the pipe (`%>%`) will be passed to rpart\n",
    "as the argument `data`.\n",
    "\n",
    "-   the class variable needs a factor (nominal) or rpart will create a\n",
    "    regression tree instead of a decision tree. Use `as.factor()` if\n",
    "    necessary.\n",
    "\n",
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f808f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart.plot)\n",
    "rpart.plot(tree_default, extra = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cac368",
   "metadata": {},
   "source": [
    "*Note:* `extra=2` prints for each leaf node the number of correctly\n",
    "classified objects from data and the total number of objects from the\n",
    "training data falling into that node (correct/total).\n",
    "\n",
    "### Create a Full Tree\n",
    "\n",
    "To create a full tree, we set the complexity parameter cp to 0 (split\n",
    "even if it does not improve the tree) and we set the minimum number of\n",
    "observations in a node needed to split to the smallest value of 2 (see:\n",
    "`?rpart.control`). *Note:* full trees overfit the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f47232",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_full <- Zoo %>% rpart(type ~., data = ., control = rpart.control(minsplit = 2, cp = 0))\n",
    "rpart.plot(tree_full, extra = 2, roundint=FALSE,\n",
    "  box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \"Or\", \"Rd\", \"Pu\")) # specify 7 colors\n",
    "tree_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc6fba",
   "metadata": {},
   "source": [
    "Training error on tree with pre-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9658f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tree_default, Zoo) %>% head ()\n",
    "\n",
    "pred <- predict(tree_default, Zoo, type=\"class\")\n",
    "head(pred)\n",
    "\n",
    "confusion_table <- with(Zoo, table(type, pred))\n",
    "confusion_table\n",
    "\n",
    "correct <- confusion_table %>% diag() %>% sum()\n",
    "correct\n",
    "error <- confusion_table %>% sum() - correct\n",
    "error\n",
    "\n",
    "accuracy <- correct / (correct + error)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7461fe6",
   "metadata": {},
   "source": [
    "Use a function for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy <- function(truth, prediction) {\n",
    "    tbl <- table(truth, prediction)\n",
    "    sum(diag(tbl))/sum(tbl)\n",
    "}\n",
    "\n",
    "accuracy(Zoo %>% pull(type), pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc863c",
   "metadata": {},
   "source": [
    "Training error of the full tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df9175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(Zoo %>% pull(type), predict(tree_full, Zoo, type=\"class\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d39e62",
   "metadata": {},
   "source": [
    "Get a confusion table with more statistics (using caret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad14309",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)\n",
    "confusionMatrix(data = pred, reference = Zoo %>% pull(type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff05fef",
   "metadata": {},
   "source": [
    "### Make Predictions for New Data\n",
    "\n",
    "Make up my own animal: A lion with feathered wings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_animal <- tibble(hair = TRUE, feathers = TRUE, eggs = FALSE,\n",
    "  milk = TRUE, airborne = TRUE, aquatic = FALSE, predator = TRUE,\n",
    "  toothed = TRUE, backbone = TRUE, breathes = TRUE, venomous = FALSE,\n",
    "  fins = FALSE, legs = 4, tail = TRUE, domestic = FALSE,\n",
    "  catsize = FALSE, type = NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e6795",
   "metadata": {},
   "source": [
    "Fix columns to be factors like in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_animal <- my_animal %>% modify_if(is.logical, factor, levels = c(TRUE, FALSE))\n",
    "my_animal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846f1cc",
   "metadata": {},
   "source": [
    "Make a prediction using the default tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tree_default , my_animal, type = \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f020915",
   "metadata": {},
   "source": [
    "## Model Evaluation with Caret\n",
    "\n",
    "The package [`caret`](https://topepo.github.io/caret/) makes preparing\n",
    "training sets, building classification (and regression) models and\n",
    "evaluation easier. A great cheat sheet can be found\n",
    "[here](https://ugoproto.github.io/ugo_r_doc/pdf/caret.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51810dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc71936",
   "metadata": {},
   "source": [
    "Cross-validation runs are independent and can be done faster in\n",
    "parallel. To enable multi-core support, `caret` uses the package\n",
    "`foreach` and you need to load a `do` backend. For Linux, you can use\n",
    "`doMC` with 4 cores. Windows needs different backend like `doParallel`\n",
    "(see `caret` cheat sheet above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linux backend\n",
    "# library(doMC)\n",
    "# registerDoMC(cores = 4)\n",
    "# getDoParWorkers()\n",
    "\n",
    "## Windows backend\n",
    "# library(doParallel)\n",
    "# cl <- makeCluster(4, type=\"SOCK\")\n",
    "# registerDoParallel(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335a058",
   "metadata": {},
   "source": [
    "Set random number generator seed to make results reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef143e1",
   "metadata": {},
   "source": [
    "### Hold out Test Data\n",
    "\n",
    "Test data is not used in the model building process and set aside purely\n",
    "for testing the model. Here, we partition data the 80% training and 20%\n",
    "testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inTrain <- createDataPartition(y = Zoo$type, p = .8, list = FALSE)\n",
    "Zoo_train <- Zoo %>% slice(inTrain)\n",
    "Zoo_test <- Zoo %>% slice(-inTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18097b5",
   "metadata": {},
   "source": [
    "### Learn a Model and Tune Hyperparameters on the Training Data\n",
    "\n",
    "The package `caret` combines training and validation for hyperparameter\n",
    "tuning into a single function called `train()`. It internally splits the\n",
    "data into training and validation sets and thus will provide you with\n",
    "error estimates for different hyperparameter settings. `trainControl` is\n",
    "used to choose how testing is performed.\n",
    "\n",
    "For rpart, train tries to tune the `cp` parameter (tree complexity)\n",
    "using accuracy to chose the best model. I set `minsplit` to 2 since we\n",
    "have not much data. **Note:** Parameters used for tuning (in this case\n",
    "`cp`) need to be set using a data.frame in the argument `tuneGrid`!\n",
    "Setting it in control will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit <- Zoo_train %>%\n",
    "  train(type ~ .,\n",
    "    data = . ,\n",
    "    method = \"rpart\",\n",
    "    control = rpart.control(minsplit = 2),\n",
    "    trControl = trainControl(method = \"cv\", number = 10),\n",
    "    tuneLength = 5)\n",
    "\n",
    "fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c70607",
   "metadata": {},
   "source": [
    "**Note:** Train has built 10 trees using the training folds for each\n",
    "value of `cp` and the reported values for accuracy and Kappa are the\n",
    "averages on the validation folds.\n",
    "\n",
    "A model using the best tuning parameters and using all the data supplied\n",
    "to `train()` is available as `fit$finalModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0039801",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpart.plot(fit$finalModel, extra = 2,\n",
    "  box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \"Or\", \"Rd\", \"Pu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230eda4c",
   "metadata": {},
   "source": [
    "caret also computes variable importance. By default it uses competing\n",
    "splits (splits which would be runners up, but do not get chosen by the\n",
    "tree) for rpart models (see `? varImp`). Toothed is the runner up for\n",
    "many splits, but it never gets chosen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "varImp(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58369c",
   "metadata": {},
   "source": [
    "Here is the variable importance without competing splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp <- varImp(fit, compete = FALSE)\n",
    "imp\n",
    "\n",
    "ggplot(imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e2d47",
   "metadata": {},
   "source": [
    "**Note:** Not all models provide a variable importance function. In this\n",
    "case caret might calculate the variable importance by itself and ignore\n",
    "the model (see `? varImp`)!\n",
    "\n",
    "## Testing: Confusion Matrix and Confidence Interval for Accuracy\n",
    "\n",
    "Use the best model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred <- predict(fit, newdata = Zoo_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebfbbb",
   "metadata": {},
   "source": [
    "Caret's `confusionMatrix()` function calculates accuracy, confidence\n",
    "intervals, kappa and many more evaluation metrics. You need to use\n",
    "separate test data to create a confusion matrix based on the\n",
    "generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10182176",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix(data = pred, ref = Zoo_test$type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1dc65a",
   "metadata": {},
   "source": [
    "**Some notes**\n",
    "\n",
    "-   Many classification algorithms and `train` in caret do not deal well\n",
    "    with missing values. If your classification model can deal with\n",
    "    missing values (e.g., `rpart`) then use `na.action = na.pass` when\n",
    "    you call `train` and `predict`. Otherwise, you need to remove\n",
    "    observations with missing values with `na.omit` or use imputation to\n",
    "    replace the missing values before you train the model. Make sure\n",
    "    that you still have enough observations left.\n",
    "-   Make sure that nominal variables (this includes logical variables)\n",
    "    are coded as factors.\n",
    "-   The class variable for train in caret cannot have level names that\n",
    "    are keywords in R (e.g., `TRUE` and `FALSE`). Rename them to, for\n",
    "    example, \"yes\" and \"no.\"\n",
    "-   Make sure that nominal variables (factors) have examples for all\n",
    "    possible values. Some methods might have problems with variable\n",
    "    values without examples. You can drop empty levels using\n",
    "    `droplevels` or `factor`.\n",
    "-   Sampling in train might create a sample that does not contain\n",
    "    examples for all values in a nominal (factor) variable. You will get\n",
    "    an error message. This most likely happens for variables which have\n",
    "    one very rare value. You may have to remove the variable.\n",
    "\n",
    "## Model Comparison\n",
    "\n",
    "We will compare decision trees with a k-nearest neighbors (kNN)\n",
    "classifier. We will create fixed sampling scheme (10-folds) so we\n",
    "compare the different models using exactly the same folds. It is\n",
    "specified as `trControl` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index <- createFolds(Zoo_train$type, k = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42829271",
   "metadata": {},
   "source": [
    "Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa32508",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpartFit <- Zoo_train %>% train(type ~ .,\n",
    "  data = .,\n",
    "  method = \"rpart\",\n",
    "  tuneLength = 10,\n",
    "  trControl = trainControl(method = \"cv\", indexOut = train_index)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bbbd5d",
   "metadata": {},
   "source": [
    "**Note:** for kNN we ask `train` to scale the data using\n",
    "`preProcess = \"scale\"`. Logicals will be used as 0-1 variables in\n",
    "Euclidean distance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnFit <- Zoo_train %>% train(type ~ .,\n",
    "  data = .,\n",
    "  method = \"knn\",\n",
    "  preProcess = \"scale\",\n",
    "\ttuneLength = 10,\n",
    "\ttrControl = trainControl(method = \"cv\", indexOut = train_index)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68777c3",
   "metadata": {},
   "source": [
    "Compare accuracy over all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "resamps <- resamples(list(\n",
    "\t\tCART = rpartFit,\n",
    "\t\tkNearestNeighbors = knnFit\n",
    "\t\t))\n",
    "\n",
    "summary(resamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cab8c6",
   "metadata": {},
   "source": [
    "`caret` provides some visualizations using the package `lattice`. For\n",
    "example, a boxplot to compare the accuracy and kappa distribution (over\n",
    "the 10 folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lattice)\n",
    "bwplot(resamps, layout = c(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bec77",
   "metadata": {},
   "source": [
    "We see that kNN is performing consistently better on the folds than CART\n",
    "(except for some outlier folds).\n",
    "\n",
    "Find out if one models is statistically better than the other (is the\n",
    "difference in accuracy is not zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f85c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "difs <- diff(resamps)\n",
    "difs\n",
    "\n",
    "summary(difs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830370c",
   "metadata": {},
   "source": [
    "p-values tells you the probability of seeing an even more extreme value\n",
    "(difference between accuracy) given that the null hypothesis (difference\n",
    "= 0) is true. For a better classifier, the p-value should be less than\n",
    ".05 or 0.01. `diff` automatically applies Bonferroni correction for\n",
    "multiple comparisons. In this case, kNN seems better but the classifiers\n",
    "do not perform statistically differently.\n",
    "\n",
    "## Feature Selection and Feature Preparation\n",
    "\n",
    "Decision trees implicitly select features for splitting, but we can also\n",
    "select features manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(FSelector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c0fc9",
   "metadata": {},
   "source": [
    "see:\n",
    "<http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection#The_Feature_Ranking_Approach>\n",
    "\n",
    "### Univariate Feature Importance Score\n",
    "\n",
    "These scores measure how related each feature is to the class variable.\n",
    "For discrete features (as in our case), the chi-square statistic can be\n",
    "used to derive a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc123ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights <- Zoo_train %>% chi.squared(type ~ ., data = .) %>%\n",
    "  as_tibble(rownames = \"feature\") %>%\n",
    "  arrange(desc(attr_importance))\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7086d2b",
   "metadata": {},
   "source": [
    "plot importance in descending order (using `reorder` to order factor\n",
    "levels used by `ggplot`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(weights,\n",
    "  aes(x = attr_importance, y = reorder(feature, attr_importance))) +\n",
    "  geom_bar(stat = \"identity\") +\n",
    "  xlab(\"Importance score\") + ylab(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7c7db",
   "metadata": {},
   "source": [
    "Get the 5 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset <- cutoff.k(weights %>% column_to_rownames(\"feature\"), 5)\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab9ffd",
   "metadata": {},
   "source": [
    "Use only the best 5 features to build a model (`Fselector` provides\n",
    "`as.simple.formula`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f <- as.simple.formula(subset, \"type\")\n",
    "f\n",
    "\n",
    "m <- Zoo_train %>% rpart(f, data = .)\n",
    "rpart.plot(m, extra = 2, roundint = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c896c12",
   "metadata": {},
   "source": [
    "There are many alternative ways to calculate univariate importance\n",
    "scores (see package FSelector). Some of them (also) work for continuous\n",
    "features. One example is the information gain ratio based on entropy as\n",
    "used in decision tree induction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc05f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zoo_train %>% gain.ratio(type ~ ., data = .) %>%\n",
    "  as_tibble(rownames = \"feature\") %>%\n",
    "  arrange(desc(attr_importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78415a10",
   "metadata": {},
   "source": [
    "### Feature Subset Selection\n",
    "\n",
    "Often features are related and calculating importance for each feature\n",
    "independently is not optimal. We can use greedy search heuristics. For\n",
    "example `cfs` uses correlation/entropy with best first search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zoo_train %>% cfs(type ~ ., data = .)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca3667",
   "metadata": {},
   "source": [
    "Black-box feature selection uses an evaluator function (the black box)\n",
    "to calculate a score to be maximized. First, we define an evaluation\n",
    "function that builds a model given a subset of features and calculates a\n",
    "quality score. We use here the average for 5 bootstrap samples\n",
    "(`method = \"cv\"` can also be used instead), no tuning (to be faster),\n",
    "and the average accuracy as the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfe76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator <- function(subset) {\n",
    "  model <- Zoo_train %>% train(as.simple.formula(subset, \"type\"),\n",
    "    data = .,\n",
    "    method = \"rpart\",\n",
    "    trControl = trainControl(method = \"boot\", number = 5),\n",
    "    tuneLength = 0)\n",
    "  results <- model$resample$Accuracy\n",
    "  cat(\"Trying features:\", paste(subset, collapse = \" + \"), \"\\n\")\n",
    "  m <- mean(results)\n",
    "  cat(\"Accuracy:\", round(m, 2), \"\\n\\n\")\n",
    "  m\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5d3e3",
   "metadata": {},
   "source": [
    "Start with all features (but not the class variable `type`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80acb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features <- Zoo_train %>% colnames() %>% setdiff(\"type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b82ea",
   "metadata": {},
   "source": [
    "There are several (greedy) search strategies available. These run for a\n",
    "while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##subset <- backward.search(features, evaluator)\n",
    "##subset <- forward.search(features, evaluator)\n",
    "##subset <- best.first.search(features, evaluator)\n",
    "##subset <- hill.climbing.search(features, evaluator)\n",
    "##subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031b2b5",
   "metadata": {},
   "source": [
    "### Using Dummy Variables for Factors\n",
    "\n",
    "Nominal features (factors) are often encoded as a series of 0-1 dummy\n",
    "variables. For example, let us try to predict if an animal is a predator\n",
    "given the type. First we use the original encoding of type as a factor\n",
    "with several values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_predator <- Zoo_train %>% rpart(predator ~ type, data = .)\n",
    "rpart.plot(tree_predator, extra = 2, roundint = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1b295",
   "metadata": {},
   "source": [
    "**Note:** Some splits use multiple values. Building the tree will become\n",
    "extremely slow if a factor has many levels (different values) since the\n",
    "tree has to check all possible splits into two subsets. This situation\n",
    "should be avoided.\n",
    "\n",
    "Convert type into a set of 0-1 dummy variables using `class2ind`. See\n",
    "also `? dummyVars` in package `caret`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed67d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zoo_train_dummy <- as_tibble(class2ind(Zoo_train$type)) %>% mutate_all(as.factor) %>%\n",
    "  add_column(predator = Zoo_train$predator)\n",
    "Zoo_train_dummy\n",
    "\n",
    "tree_predator <- Zoo_train_dummy %>% rpart(predator ~ ., data = .,\n",
    "  control = rpart.control(minsplit = 2, cp = 0.01))\n",
    "rpart.plot(tree_predator, roundint = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020bb25",
   "metadata": {},
   "source": [
    "Using `caret` on the original factor encoding automatically translates\n",
    "factors (here type) into 0-1 dummy variables (e.g., `typeinsect = 0`).\n",
    "The reason is that some models cannot directly use factors and `caret`\n",
    "tries to consistently work with all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ffe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit <- Zoo_train %>% train(predator ~ type, data = ., method = \"rpart\",\n",
    "  control = rpart.control(minsplit = 2),\n",
    "  tuneGrid = data.frame(cp = 0.01))\n",
    "fit\n",
    "\n",
    "rpart.plot(fit$finalModel, extra = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df5031",
   "metadata": {},
   "source": [
    "*Note:* To use a fixed value for the tuning parameter `cp`, we have to\n",
    "create a tuning grid that only contains that value.\n",
    "\n",
    "## Class Imbalance\n",
    "\n",
    "Classifiers have a hard time to learn from data where we have much more\n",
    "observations for one class (called the majority class). This is called\n",
    "the class imbalance problem.\n",
    "\n",
    "Here is a very good [article about the problem and\n",
    "solutions.](http://www.kdnuggets.com/2016/08/learning-from-imbalanced-classes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7870236",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "data(Zoo, package=\"mlbench\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c225e",
   "metadata": {},
   "source": [
    "Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff551980",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(Zoo, aes(y = type)) + geom_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2b5d6",
   "metadata": {},
   "source": [
    "To create an imbalanced problem, we want to decide if an animal is an\n",
    "reptile. First, we change the class variable to make it into a binary\n",
    "reptile/no reptile classification problem. **Note:** We use here the\n",
    "training data for testing. You should use a separate testing data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137532d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zoo_reptile <- Zoo %>% mutate(\n",
    "  type = factor(Zoo$type == \"reptile\", levels = c(FALSE, TRUE),\n",
    "    labels = c(\"nonreptile\", \"reptile\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a78e4",
   "metadata": {},
   "source": [
    "Do not forget to make the class variable a factor (a nominal variable)\n",
    "or you will get a regression tree instead of a classification tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8223e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Zoo_reptile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e063668",
   "metadata": {},
   "source": [
    "See if we have a class imbalance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90248a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(Zoo_reptile, aes(y = type)) + geom_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c390e8",
   "metadata": {},
   "source": [
    "Create test and training data. I use here a 50/50 split to make sure\n",
    "that the test set has some samples of the rare reptile class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfe303",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "\n",
    "inTrain <- createDataPartition(y = Zoo_reptile$type, p = .5, list = FALSE)\n",
    "training_reptile <- Zoo_reptile %>% slice(inTrain)\n",
    "testing_reptile <- Zoo_reptile %>% slice(-inTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe0f13",
   "metadata": {},
   "source": [
    "the new class variable is clearly not balanced. This is a problem for\n",
    "building a tree!\n",
    "\n",
    "### Option 1: Use the Data As Is and Hope For The Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ac69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit <- training_reptile %>% train(type ~ .,\n",
    "  data = .,\n",
    "  method = \"rpart\",\n",
    "  trControl = trainControl(method = \"cv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0314549",
   "metadata": {},
   "source": [
    "**Warnings:** \"There were missing values in resampled performance\n",
    "measures.\" means that some test folds did not contain examples of both\n",
    "classes. This is very likely with class imbalance and small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit\n",
    "rpart.plot(fit$finalModel, extra = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68083121",
   "metadata": {},
   "source": [
    "the tree predicts everything as non-reptile. Have a look at the error on\n",
    "the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix(data = predict(fit, testing_reptile),\n",
    "  ref = testing_reptile$type, positive = \"reptile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca67ff",
   "metadata": {},
   "source": [
    "Accuracy is high, but it is exactly the same as the no-information rate\n",
    "and kappa is zero. Sensitivity is also zero, meaning that we do not\n",
    "identify any positive (reptile). If the cost of missing a positive is\n",
    "much larger than the cost associated with misclassifying a negative,\n",
    "then accuracy is not a good measure! By dealing with imbalance, we are\n",
    "**not** concerned with accuracy, but we want to increase the\n",
    "sensitivity, i.e., the chance to identify positive examples.\n",
    "\n",
    "**Note:** The positive class value (the one that you want to detect) is\n",
    "set manually to reptile using `positive = \"reptile\"`. Otherwise\n",
    "sensitivity/specificity will not be correctly calculated.\n",
    "\n",
    "### Option 2: Balance Data With Resampling\n",
    "\n",
    "We use stratified sampling with replacement (to oversample the\n",
    "minority/positive class). You could also use SMOTE (in package **DMwR**)\n",
    "or other sampling strategies (e.g., from package **unbalanced**). We use\n",
    "50+50 observations here (**Note:** many samples will be chosen several\n",
    "times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(sampling)\n",
    "set.seed(1000) # for repeatability\n",
    "\n",
    "id <- strata(training_reptile, stratanames = \"type\", size = c(50, 50), method = \"srswr\")\n",
    "training_reptile_balanced <- training_reptile %>% slice(id$ID_unit)\n",
    "table(training_reptile_balanced$type)\n",
    "\n",
    "fit <- training_reptile_balanced %>% train(type ~ .,\n",
    "  data = .,\n",
    "  method = \"rpart\",\n",
    "  trControl = trainControl(method = \"cv\"),\n",
    "  control = rpart.control(minsplit = 5))\n",
    "\n",
    "fit\n",
    "rpart.plot(fit$finalModel, extra = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea52c1",
   "metadata": {},
   "source": [
    "Check on the unbalanced testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix(data = predict(fit, testing_reptile),\n",
    "  ref = testing_reptile$type, positive = \"reptile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d994fa",
   "metadata": {},
   "source": [
    "**Note** that the accuracy is below the no information rate! However,\n",
    "kappa (improvement of accuracy over randomness) and sensitivity (the\n",
    "ability to identify reptiles) have increased.\n",
    "\n",
    "There is a tradeoff between sensitivity and specificity (how many of the\n",
    "identified animals are really reptiles) The tradeoff can be controlled\n",
    "using the sample proportions. We can sample more reptiles to increase\n",
    "sensitivity at the cost of lower specificity (this effect cannot be seen\n",
    "in the data since the test set has only a few reptiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id <- strata(training_reptile, stratanames = \"type\", size = c(50, 100), method = \"srswr\")\n",
    "training_reptile_balanced <- training_reptile %>% slice(id$ID_unit)\n",
    "table(training_reptile_balanced$type)\n",
    "\n",
    "fit <- training_reptile_balanced %>% train(type ~ .,\n",
    "  data = .,\n",
    "  method = \"rpart\",\n",
    "  trControl = trainControl(method = \"cv\"),\n",
    "  control = rpart.control(minsplit = 5))\n",
    "\n",
    "confusionMatrix(data = predict(fit, testing_reptile),\n",
    "  ref = testing_reptile$type, positive = \"reptile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6b059",
   "metadata": {},
   "source": [
    "### Option 3: Build A Larger Tree and use Predicted Probabilities\n",
    "\n",
    "Increase complexity and require less data for splitting a node. Here I\n",
    "also use AUC (area under the ROC) as the tuning metric. You need to\n",
    "specify the two class summary function. Note that the tree still trying\n",
    "to improve accuracy on the data and not AUC! I also enable class\n",
    "probabilities since I want to predict probabilities later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit <- training_reptile %>% train(type ~ .,\n",
    "  data = .,\n",
    "  method = \"rpart\",\n",
    "  tuneLength = 10,\n",
    "  trControl = trainControl(method = \"cv\",\n",
    "    classProbs = TRUE,                 ## necessary for predict with type=\"prob\"\n",
    "    summaryFunction=twoClassSummary),  ## necessary for ROC\n",
    "  metric = \"ROC\",\n",
    "  control = rpart.control(minsplit = 3))\n",
    "fit\n",
    "\n",
    "rpart.plot(fit$finalModel, extra = 2)\n",
    "\n",
    "confusionMatrix(data = predict(fit, testing_reptile),\n",
    "  ref = testing_reptile$type, positive = \"reptile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24072053",
   "metadata": {},
   "source": [
    "**Note:** Accuracy is high, but it is close or below to the\n",
    "no-information rate!\n",
    "\n",
    "#### Create A Biased Classifier\n",
    "\n",
    "We can create a classifier which will detect more reptiles at the\n",
    "expense of misclassifying non-reptiles. This is equivalent to increasing\n",
    "the cost of misclassifying a reptile as a non-reptile. The usual rule is\n",
    "to predict in each node the majority class from the test data in the\n",
    "node. For a binary classification problem that means a probability of\n",
    "\\>50%. In the following, we reduce this threshold to 1% or more. This\n",
    "means that if the new observation ends up in a leaf node with 1% or more\n",
    "reptiles from training then the observation will be classified as a\n",
    "reptile. The data set is small and this works better with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob <- predict(fit, testing_reptile, type = \"prob\")\n",
    "tail(prob)\n",
    "pred <- as.factor(ifelse(prob[,\"reptile\"]>=0.01, \"reptile\", \"nonreptile\"))\n",
    "\n",
    "confusionMatrix(data = pred,\n",
    "  ref = testing_reptile$type, positive = \"reptile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3e57d",
   "metadata": {},
   "source": [
    "**Note** that accuracy goes down and is below the no information rate.\n",
    "However, both measures are based on the idea that all errors have the\n",
    "same cost. What is important is that we are now able to find more\n",
    "reptiles.\n",
    "\n",
    "#### Plot the ROC Curve\n",
    "\n",
    "Since we have a binary classification problem and a classifier that\n",
    "predicts a probability for an observation to be a reptile, we can also\n",
    "use a [receiver operating characteristic\n",
    "(ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "curve. For the ROC curve all different cutoff thresholds for the\n",
    "probability are used and then connected with a line. The area under the\n",
    "curve represents a single number for how well the classifier works (the\n",
    "closer to one, the better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"pROC\")\n",
    "r <- roc(testing_reptile$type == \"reptile\", prob[,\"reptile\"])\n",
    "r\n",
    "\n",
    "ggroc(r) + geom_abline(intercept = 1, slope = 1, color = \"darkgrey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31bbcf",
   "metadata": {},
   "source": [
    "### Option 4: Use a Cost-Sensitive Classifier\n",
    "\n",
    "The implementation of CART in `rpart` can use a cost matrix for making\n",
    "splitting decisions (as parameter `loss`). The matrix has the form\n",
    "\n",
    "TP FP FN TN\n",
    "\n",
    "TP and TN have to be 0. We make FN very expensive (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost <- matrix(c(\n",
    "  0,   1,\n",
    "  100, 0\n",
    "), byrow = TRUE, nrow = 2)\n",
    "cost\n",
    "\n",
    "fit <- training_reptile %>% train(type ~ .,\n",
    "  data = .,\n",
    "  method = \"rpart\",\n",
    "  parms = list(loss = cost),\n",
    "  trControl = trainControl(method = \"cv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6af6d",
   "metadata": {},
   "source": [
    "The warning \"There were missing values in resampled performance\n",
    "measures\" means that some folds did not contain any reptiles (because of\n",
    "the class imbalance) and thus the performance measures could not be\n",
    "calculates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit\n",
    "\n",
    "rpart.plot(fit$finalModel, extra = 2)\n",
    "\n",
    "confusionMatrix(data = predict(fit, testing_reptile),\n",
    "  ref = testing_reptile$type, positive = \"reptile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0cbaf",
   "metadata": {},
   "source": [
    "The high cost for false negatives results in a classifier that does not\n",
    "miss any reptile.\n",
    "\n",
    "**Note:** Using a cost-sensitive classifier is often the best option.\n",
    "Unfortunately, the most classification algorithms (or their\n",
    "implementation) do not have the ability to consider misclassification\n",
    "cost."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,name,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
