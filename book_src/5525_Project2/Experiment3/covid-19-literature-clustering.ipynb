{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> COVID-19 Literature Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Goal\n",
    "Given the rapidly growing amount of literature on COVID-19, it is difficult to keep up with the major research trends being explored on this topic. Can we cluster similar research articles to make it easier for health professionals to find relevant research trends? \n",
    "\n",
    "### Dataset we will be using\n",
    "\n",
    ">*In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.*\n",
    "#### Cite: [COVID-19 Open Research Dataset Challenge (CORD-19) | Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "The data loading procedure described below is adapted from the following notebook by Ivan Ega Pratama, from Kaggle [Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool). Before running the procedures below, make sure to unzip the file CORD-19-research-challenge.zip inside the Experiment3 folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the metadata of the articles. 'title' and 'journal' attributes may be useful later when we cluster the articles to see what kinds of articles cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'CORD-19-research-challenge/'\n",
    "metadata_path = f'{root_path}/metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch All of JSON File Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get the paths to all JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "len(all_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We need the following helper functions for reading files and adding breaks after every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            # Abstract\n",
    "            for entry in content['abstract']:\n",
    "                self.abstract.append(entry['text'])\n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                self.body_text.append(entry['text'])\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n",
    "    \n",
    "# Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen.\n",
    "    \n",
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using theses helper functions, let's read in the articles into a DataFrame that can be used easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "for idx, entry in enumerate(all_json):\n",
    "    try:\n",
    "        if idx % (len(all_json) // 10) == 0:\n",
    "            print(f'Processing index: {idx} of {len(all_json)}')\n",
    "        content = FileReader(entry)\n",
    "\n",
    "        # get metadata information\n",
    "        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "        # no metadata, skip this paper\n",
    "        if len(meta_data) == 0:\n",
    "            continue\n",
    "\n",
    "        dict_['paper_id'].append(content.paper_id)\n",
    "        dict_['abstract'].append(content.abstract)\n",
    "        dict_['body_text'].append(content.body_text)\n",
    "\n",
    "        # also create a column for the summary of abstract to be used in a plot\n",
    "        if len(content.abstract) == 0: \n",
    "            # no abstract provided\n",
    "            dict_['abstract_summary'].append(\"Not provided.\")\n",
    "        elif len(content.abstract.split(' ')) > 100:\n",
    "            # abstract provided is too long for plot, take first 300 words append with ...\n",
    "            info = content.abstract.split(' ')[:100]\n",
    "            summary = get_breaks(' '.join(info), 40)\n",
    "            dict_['abstract_summary'].append(summary + \"...\")\n",
    "        else:\n",
    "            # abstract is short enough\n",
    "            summary = get_breaks(content.abstract, 40)\n",
    "            dict_['abstract_summary'].append(summary)\n",
    "\n",
    "        # get metadata information\n",
    "        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "\n",
    "        try:\n",
    "            # if more than one author\n",
    "            authors = meta_data['authors'].values[0].split(';')\n",
    "            if len(authors) > 2:\n",
    "                # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n",
    "                dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n",
    "            else:\n",
    "                # authors will fit in plot\n",
    "                dict_['authors'].append(\". \".join(authors))\n",
    "        except Exception as e:\n",
    "            # if only one author - or Null valie\n",
    "            dict_['authors'].append(meta_data['authors'].values[0])\n",
    "\n",
    "        # add the title information, add breaks when needed\n",
    "        try:\n",
    "            title = get_breaks(meta_data['title'].values[0], 40)\n",
    "            dict_['title'].append(title)\n",
    "        # if title was not provided\n",
    "        except Exception as e:\n",
    "            dict_['title'].append(meta_data['title'].values[0])\n",
    "\n",
    "        # add the journal information\n",
    "        dict_['journal'].append(meta_data['journal'].values[0])\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        continue\n",
    "    \n",
    "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the Word Count Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add two extra columns related to the word count of the abstract and body_text, which  can be useful features later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\n",
    "df_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Possible Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the unique values above, we can see that there are duplicates. It may have caused because of author submiting the article to multiple journals. Let's remove the duplicates from our dataset, and also those articles that are missing abstracts or body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.dropna(inplace=True)\n",
    "df_covid = df_covid[df_covid.abstract != ''] #Remove rows which are missing abstracts\n",
    "df_covid = df_covid[df_covid.body_text != ''] #Remove rows which are missing body_text\n",
    "df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True) # remove duplicate rows having same abstract and body_text\n",
    "df_covid.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us limit the number of articles to speed up computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = df_covid.head(12500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove punctuation from each text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each text to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(input_str):\n",
    "    input_str = input_str.lower()\n",
    "    return input_str\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(lambda x: lower_case(x))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: lower_case(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the text cleaned up, we can create our features vector which can be fed into a clustering or dimensionality reduction algorithm. For our first try, we will focus on the text on the body of the articles. Let's grab that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_covid.drop([\"paper_id\", \"abstract\", \"abstract_word_count\", \"body_word_count\", \"authors\", \"title\", \"journal\", \"abstract_summary\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform this 1D DataFrame into a 1D list where each index is an article (instance), so that we can work with words from each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_arr = text.stack().tolist()\n",
    "len(text_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a 2D list, where each row is an instance and each column is a word. Meaning, we will separate each instance into words:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for ii in range(0,len(text)):\n",
    "    words.append(str(text.iloc[ii]['body_text']).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want now is to create n-grams from the words with n=2 (i.e., 2-grams). A 2-gram is a sequence of two words appearing together (e.g., 'thank you'). The motivation behind using 2-grams is to describe a document using pairs of consecutive words, instead of individual words, so as to capture the co-occurrence information of words at adjacent positions in a document. We have created a 2D list where each row is an instance (or document) and every column is a word. We need to create a similar 2D list where every row is an instance and every column is a 2-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_all = []\n",
    "\n",
    "for word in words:\n",
    "    # get n-grams for the instance\n",
    "    n_gram = []\n",
    "    for i in range(len(word)-2+1):\n",
    "        n_gram.append(\"\".join(word[i:i+2]))\n",
    "    n_gram_all.append(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_all[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize with HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize the set of n-gram features constructed for every document, we will use the in-built HashVectorizer function. We will limit the feature size to 2^12 (4096) to speed up computations. We might need to increase this later to improve the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# hash vectorizer instance\n",
    "hvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)\n",
    "\n",
    "# features matrix X\n",
    "X = hvec.fit_transform(n_gram_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with t-SNE\n",
    "We first reduce the dimensionality of X from a 4096-dimensional space to a 2-dimensional space using a popular non-linear dimensionality reduction technique called t-SNE. In this process, t-SNE will keep similar instances together while trying to push different instances far from each other. Resulting 2-D scatter plot of t-SNE features can be useful to see which articles cluster near each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following cell may take 20-30 minutes to run\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1, perplexity=5)\n",
    "X_embedded = tsne.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", 1)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n",
    "\n",
    "plt.title(\"t-SNE Covid-19 Articles\")\n",
    "# plt.savefig(\"plots/t-sne_covid19.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see few clusters forming. However, without labels, it is difficult to see the clusters. Let's try if we can use K-Means to generate our labels for these clusters. We can later use this information to produce a scatterplot with labels to verify the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Clustering with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply K-means with K = 10. For computational efficiency, we will use the Minibatch version of Kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "k = 10\n",
    "kmeans = MiniBatchKMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the labels, let's plot the t-SNE scatterplot again and see if K-means is able to capture the pattern of clusters in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", len(set(y_pred)))\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
    "plt.title(\"t-SNE Covid-19 Articles - Clustered\")\n",
    "# plt.savefig(\"plots/t-sne_covid19_label.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty promising. We can see that articles from the same cluster are near each other, forming groups. There are still overlaps, though. So we will have to see if we can improve this by changing the number of clusters (K), using another clustering algorithm, or using a different feature size in the HashingVectorizer. We can also consider trying 3-grams, 4-grams, or 1-gram (plain text) instead of 2-grams to create features and vectorize them using different document vectorization methods, e.g., HashVectorizer or Tf-idfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Using Tf-idf with Plain Text\n",
    "Let's see if we will be able to get better clusters using plain text as instances rather than 2-grams and vectorize it using Tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2**12)\n",
    "X = vectorizer.fit_transform(df_covid['body_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans with Plain text and Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's try to get our cluster labels. We will choose 10 clusters again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "k = 10\n",
    "kmeans = MiniBatchKMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with t-SNE (Plain text and Tf-idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the dimensionality using t-SNE again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following cell will take 20-30 minutes to run\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1)\n",
    "X_embedded = tsne.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", len(set(y)))\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\n",
    "plt.title(\"t-SNE Covid-19 Articles - Clustered(K-Means) - Tf-idf with Plain Text\")\n",
    "# plt.savefig(\"plots/t-sne_covid19_label_TFID.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are able to see the clusters more clearly, as the clusters are further apart from each other. We can also start to see that there is possibly more than 10 clusters we need to identify using k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with PCA (Plain text and Tf-idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE doesn't scale well. This is why the run-time of this Notebook is about 40 minutes to 1 hour with an average computer. Let's try to see if we can achieve reasonable results with PCA as it scales very well with larger datasets and dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", len(set(y)))\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(pca_result[:,0], pca_result[:,1], hue=y, legend='full', palette=palette)\n",
    "plt.title(\"PCA Covid-19 Articles - Clustered (K-Means) - Tf-idf with Plain Text\")\n",
    "# plt.savefig(\"plots/pca_covid19_label_TFID.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it may be easier to see the results in a 3 dimensional plot. So let's try to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=pca_result[:,0], \n",
    "    ys=pca_result[:,1], \n",
    "    zs=pca_result[:,2], \n",
    "    c=y, \n",
    "    cmap='tab10'\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "plt.title(\"PCA Covid-19 Articles (3D) - Clustered (K-Means) - Tf-idf with Plain Text\")\n",
    "# plt.savefig(\"plots/pca_covid19_label_TFID_3d.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Clusters?\n",
    "On our previous plot we could see that there is more clusters than only 10. Let's try to label them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "k = 20\n",
    "kmeans = MiniBatchKMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "y = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import random \n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# let's shuffle the list so distinct colors stay next to each other\n",
    "palette = sns.hls_palette(20, l=.4, s=.9)\n",
    "random.shuffle(palette)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\n",
    "plt.title(\"t-SNE Covid-19 Articles - Clustered(K-Means) - Tf-idf with Plain Text\")\n",
    "# plt.savefig(\"plots/t-sne_covid19_20label_TFID.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be helpful if we have a demo tool that can be used to see what articles are identified as similar using our Clustering and Dimensionality Reduction methods, right? Let's put together a interactive scatter plot of t-SNE to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.transform import transform\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import RadioButtonGroup\n",
    "from bokeh.models import TextInput\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Div\n",
    "from bokeh.models import Paragraph\n",
    "from bokeh.layouts import column, widgetbox\n",
    "\n",
    "output_notebook()\n",
    "y_labels = y_pred\n",
    "\n",
    "# data sources\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x= X_embedded[:,0], \n",
    "    y= X_embedded[:,1],\n",
    "    x_backup = X_embedded[:,0],\n",
    "    y_backup = X_embedded[:,1],\n",
    "    desc= y_labels, \n",
    "    titles= df_covid['title'],\n",
    "    authors = df_covid['authors'],\n",
    "    journal = df_covid['journal'],\n",
    "    abstract = df_covid['abstract_summary'],\n",
    "    labels = [\"C-\" + str(x) for x in y_labels]\n",
    "    ))\n",
    "\n",
    "# hover over information\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Title\", \"@titles{safe}\"),\n",
    "    (\"Author(s)\", \"@authors\"),\n",
    "    (\"Journal\", \"@journal\"),\n",
    "    (\"Abstract\", \"@abstract{safe}\"),\n",
    "],\n",
    "                 point_policy=\"follow_mouse\")\n",
    "\n",
    "# map colors\n",
    "mapper = linear_cmap(field_name='desc', \n",
    "                     palette=Category20[20],\n",
    "                     low=min(y_labels) ,high=max(y_labels))\n",
    "\n",
    "# prepare the figure\n",
    "p = figure(plot_width=800, plot_height=800, \n",
    "           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n",
    "           title=\"t-SNE Covid-19 Articles, Clustered(K-Means), Tf-idf with Plain Text\", \n",
    "           toolbar_location=\"right\")\n",
    "\n",
    "# plot\n",
    "p.scatter('x', 'y', size=5, \n",
    "          source=source,\n",
    "          fill_color=mapper,\n",
    "          line_alpha=0.3,\n",
    "          line_color=\"black\",\n",
    "          legend = 'labels')\n",
    "\n",
    "# add callback to control \n",
    "callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n",
    "            \n",
    "            var radio_value = cb_obj.active;\n",
    "            var data = source.data; \n",
    "            \n",
    "            x = data['x'];\n",
    "            y = data['y'];\n",
    "            \n",
    "            x_backup = data['x_backup'];\n",
    "            y_backup = data['y_backup'];\n",
    "            \n",
    "            labels = data['desc'];\n",
    "            \n",
    "            if (radio_value == '20') {\n",
    "                for (i = 0; i < x.length; i++) {\n",
    "                    x[i] = x_backup[i];\n",
    "                    y[i] = y_backup[i];\n",
    "                }\n",
    "            }\n",
    "            else {\n",
    "                for (i = 0; i < x.length; i++) {\n",
    "                    if(labels[i] == radio_value) {\n",
    "                        x[i] = x_backup[i];\n",
    "                        y[i] = y_backup[i];\n",
    "                    } else {\n",
    "                        x[i] = undefined;\n",
    "                        y[i] = undefined;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "        source.change.emit();\n",
    "        \"\"\")\n",
    "\n",
    "# callback for searchbar\n",
    "keyword_callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n",
    "            \n",
    "            var text_value = cb_obj.value;\n",
    "            var data = source.data; \n",
    "            \n",
    "            x = data['x'];\n",
    "            y = data['y'];\n",
    "            \n",
    "            x_backup = data['x_backup'];\n",
    "            y_backup = data['y_backup'];\n",
    "            \n",
    "            abstract = data['abstract'];\n",
    "            titles = data['titles'];\n",
    "            authors = data['authors'];\n",
    "            journal = data['journal'];\n",
    "\n",
    "            for (i = 0; i < x.length; i++) {\n",
    "                if(abstract[i].includes(text_value) || \n",
    "                   titles[i].includes(text_value) || \n",
    "                   authors[i].includes(text_value) || \n",
    "                   journal[i].includes(text_value)) {\n",
    "                    x[i] = x_backup[i];\n",
    "                    y[i] = y_backup[i];\n",
    "                } else {\n",
    "                    x[i] = undefined;\n",
    "                    y[i] = undefined;\n",
    "                }\n",
    "            }\n",
    "            \n",
    "\n",
    "\n",
    "        source.change.emit();\n",
    "        \"\"\")\n",
    "\n",
    "# option\n",
    "option = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n",
    "                                  \"C-3\", \"C-4\", \"C-5\",\n",
    "                                  \"C-6\", \"C-7\", \"C-8\",\n",
    "                                  \"C-9\", \"C-10\", \"C-11\",\n",
    "                                  \"C-12\", \"C-13\", \"C-14\",\n",
    "                                  \"C-15\", \"C-16\", \"C-17\",\n",
    "                                  \"C-18\", \"C-19\", \"All\"], \n",
    "                          active=20, callback=callback)\n",
    "\n",
    "# search box\n",
    "keyword = TextInput(title=\"Search:\", callback=keyword_callback)\n",
    "\n",
    "#header\n",
    "header = Div(text=\"\"\"<h1>COVID-19 Literature Cluster</h1>\"\"\")\n",
    "\n",
    "# show\n",
    "show(column(header, widgetbox(option, keyword),p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please see the tools on right top.\n",
    "#### If the text doesn't fit in the screen on the above plot when you hover, please try the 'Box Zoom' tool to zoom to the area where the target plot is. This will help the hover message to fit inside the screen. \n",
    "#### Use the 'Reset' button to revert the zoom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an adaption from the following kaggle notebook https://www.kaggle.com/maksimeren/covid-19-literature-clustering\n",
    "### You can find the full version of the interactive plot here on GitHub: \n",
    "#### https://maksimekin.github.io/COVID19-Literature-Clustering/plots/t-sne_covid-19_interactive.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
